{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf33997",
   "metadata": {},
   "source": [
    "# demo.ipynb を改変\n",
    "\n",
    "ここでは、\n",
    "\n",
    "アノテーション（point, bbox, mask）全てを画像で与えるコードを作成する\n",
    "\n",
    "demo.ipynbではpoint, bboxは座標としてコードの中で明示的に与えられていた。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "944f714c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rinotsuka/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 252\u001b[39m\n\u001b[32m    226\u001b[39m bboxes = [\n\u001b[32m    227\u001b[39m     {\n\u001b[32m    228\u001b[39m         \u001b[32m18\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m    237\u001b[39m     }\n\u001b[32m    238\u001b[39m ]\n\u001b[32m    239\u001b[39m \u001b[38;5;66;03m# Convert bbox in LA format\u001b[39;00m\n\u001b[32m    240\u001b[39m bboxes = [\n\u001b[32m    241\u001b[39m     {\n\u001b[32m    242\u001b[39m         cat_id: [\n\u001b[32m    243\u001b[39m             prompts_processor.convert_bbox(\n\u001b[32m    244\u001b[39m                 bbox,\n\u001b[32m    245\u001b[39m                 *image.size,\n\u001b[32m    246\u001b[39m                 noise=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    247\u001b[39m             )\n\u001b[32m    248\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m cat_bboxes\n\u001b[32m    249\u001b[39m         ]\n\u001b[32m    250\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m cat_id, cat_bboxes \u001b[38;5;129;01min\u001b[39;00m img_bboxes.items()\n\u001b[32m    251\u001b[39m     }\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m img_bboxes, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(bboxes, \u001b[43mimages\u001b[49m)\n\u001b[32m    253\u001b[39m ]\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Add background (class -1)\u001b[39;00m\n\u001b[32m    255\u001b[39m bboxes = [{**{-\u001b[32m1\u001b[39m: []}, **bboxes} \u001b[38;5;28;01mfor\u001b[39;00m bboxes \u001b[38;5;129;01min\u001b[39;00m bboxes]\n",
      "\u001b[31mNameError\u001b[39m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "sys.path.append(str(Path.cwd().parent / 'label_anything'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import wget\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import safetensors\n",
    "\n",
    "# Pretty visualizations of tensors\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "from lovely_numpy import lo\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "from label_anything import LabelAnything\n",
    "from label_anything.models import build_lam_vit_b\n",
    "from label_anything.data import get_preprocessing\n",
    "from label_anything.data.transforms import PromptsProcessor\n",
    "from label_anything.data import utils\n",
    "from label_anything.utils.utils import torch_dict_load\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define some helper function\n",
    "colors = [\n",
    "    # yellow\n",
    "    (255, 255, 0),\n",
    "    # red\n",
    "    (255, 0, 0),\n",
    "    # green\n",
    "    (0, 255, 0),\n",
    "    # blue\n",
    "    (0, 0, 255),\n",
    "    # purple\n",
    "    (255, 0, 255),\n",
    "    # cyan\n",
    "    (0, 255, 255),\n",
    "    # orange\n",
    "    (255, 165, 0),\n",
    "    # pink\n",
    "    (255, 192, 203),\n",
    "    # brown\n",
    "    (139, 69, 19),\n",
    "    # grey\n",
    "    (128, 128, 128),\n",
    "    # black\n",
    "    (0, 0, 0)\n",
    "]*3\n",
    "\n",
    "def draw_masks(img: Image, masks: torch.Tensor, colors):\n",
    "    # here masks is a dict having category names as keys\n",
    "    # associated to a list of binary masks\n",
    "    masked_image = resize(img.copy(), 256)\n",
    "    \n",
    "    for i, mask in enumerate(masks):\n",
    "        mask = mask.numpy()\n",
    "        masked_image = np.where(np.repeat(mask[:, :, np.newaxis], 3, axis=2),\n",
    "                                np.asarray(colors[i], dtype=\"uint8\"),\n",
    "                                masked_image)\n",
    "    \n",
    "    masked_image = masked_image.astype(np.uint8)\n",
    "    return cv2.addWeighted(np.array(resize(img, 256)), 0.3, masked_image, 0.7, 0)\n",
    "\n",
    "def draw_boxes(img: Image, boxes: torch.Tensor, colors):\n",
    "    img = np.array(img)\n",
    "    \n",
    "    for i, cat in enumerate(boxes):\n",
    "        for box in cat:\n",
    "            x1, y1, x2, y2 = box\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            img = cv2.rectangle(img, (x1, y1), (x2, y2), colors[i], 2)\n",
    "    return img\n",
    "\n",
    "def draw_points(img: Image, points: torch.Tensor, colors):\n",
    "    img = np.array(img)\n",
    "    \n",
    "    for i, cat in enumerate(points):\n",
    "        for point in cat:\n",
    "            x, y = point\n",
    "            x, y = int(x), int(y)\n",
    "            img = cv2.circle(img, (x, y), 5, colors[i], -1)\n",
    "    return img\n",
    "\n",
    "def draw_all(img: Image, masks, boxes, points, colors):\n",
    "    segmented_image = draw_masks(img, masks, colors)\n",
    "    img = Image.fromarray(segmented_image)\n",
    "    img = resize(img, 1024)\n",
    "    img = draw_boxes(img, boxes, colors)\n",
    "    img = Image.fromarray(img)\n",
    "    img = draw_points(img, points, colors)\n",
    "    img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "def get_image(image_tensor):\n",
    "    MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "    STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "    unnormalized_image = (image_tensor.numpy() * np.array(STD)[:, None, None]) + np.array(MEAN)[:, None, None]\n",
    "    unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "    return Image.fromarray(unnormalized_image)\n",
    "\n",
    "def dict_to_device(d, device):\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            d[k] = v.to(device)\n",
    "        if isinstance(v, dict):\n",
    "            d[k] = dict_to_device(v, device)\n",
    "        if isinstance(v, list):\n",
    "            d[k] = [dict_to_device(vv, device) for vv in v]\n",
    "        else:\n",
    "            pass\n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the Label Anything model\n",
    "image_size = 1024\n",
    "custom_preprocess = True\n",
    "DEMO_PATH = \"demo\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define Label Anything parameters and initialize it\n",
    "la = LabelAnything.from_pretrained(\"pasqualedem/label_anything_sam_1024_coco\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load images\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "img_dir = Path.cwd() / \"images\"\n",
    "img_paths = sorted(img_dir.glob(\"*.jpg\")) + sorted(img_dir.glob(\"*.png\")) + sorted(img_dir.glob(\"*.jpeg\")) + sorted(img_dir.glob(\"*.JPG\"))\n",
    "\n",
    "def open_rgb(p):\n",
    "    img = Image.open(p)\n",
    "    return img.convert(\"RGB\")   # ← ここが重要（RGBA等→RGB）\n",
    "\n",
    "# 例：先頭をクエリ、残りをサポートに\n",
    "query_image = open_rgb(img_paths[0])\n",
    "support_images = [open_rgb(p) for p in img_paths[1:]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess images\n",
    "preprocess = get_preprocessing(\n",
    "    {\n",
    "        \"common\": {\n",
    "            \"custom_preprocess\": custom_preprocess,\n",
    "            \"image_size\": image_size,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "query_image = preprocess(query_image)\n",
    "support_images = [preprocess(img) for img in support_images]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the prompt processor\n",
    "prompts_processor = PromptsProcessor(\n",
    "            long_side_length=image_size,\n",
    "            masks_side_length=256,\n",
    "            custom_preprocess=custom_preprocess,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Use Points, Bboxes and masks\n",
    "# Load the mask\n",
    "mask = Image.open(f\"{DEMO_PATH}/mask.png\")\n",
    "mask = np.array(mask)\n",
    "masks = [\n",
    "    {\n",
    "        18: [],\n",
    "        1: [mask],\n",
    "    }\n",
    "]\n",
    "lo(mask).chans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "points = [\n",
    "    {\n",
    "        18: [],\n",
    "        1: [\n",
    "            [370, 100],\n",
    "            [380, 160],\n",
    "            [375, 200],\n",
    "            ],\n",
    "    }\n",
    "]\n",
    "\n",
    "bboxes = [\n",
    "    {\n",
    "        18: [\n",
    "            [\n",
    "                202,\n",
    "                345,\n",
    "                270,\n",
    "                255,\n",
    "            ]\n",
    "        ],\n",
    "        1: [[13, 55, 181, 498]],\n",
    "    }\n",
    "]\n",
    "# Convert bbox in LA format\n",
    "bboxes = [\n",
    "    {\n",
    "        cat_id: [\n",
    "            prompts_processor.convert_bbox(\n",
    "                bbox,\n",
    "                *image.size,\n",
    "                noise=False,\n",
    "            )\n",
    "            for bbox in cat_bboxes\n",
    "        ]\n",
    "        for cat_id, cat_bboxes in img_bboxes.items()\n",
    "    }\n",
    "    for img_bboxes, image in zip(bboxes, images)\n",
    "]\n",
    "# Add background (class -1)\n",
    "bboxes = [{**{-1: []}, **bboxes} for bboxes in bboxes]\n",
    "points = [{**{-1: []}, **points} for points in points]\n",
    "masks = [{**{-1: []}, **masks} for masks in masks]\n",
    "\n",
    "# convert the lists of prompts to arrays\n",
    "for i in range(len(bboxes)):\n",
    "    for cat_id in cat_ids:\n",
    "        bboxes[i][cat_id] = np.array((bboxes[i][cat_id]))\n",
    "        points[i][cat_id] = np.array((points[i][cat_id]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Here bboxes and points are stacked and padded in a single tensor\n",
    "# flag_bboxes and flag_points used to tell the model if point is present or is padding\n",
    "bboxes, flag_bboxes = utils.annotations_to_tensor(\n",
    "            prompts_processor, bboxes, img_sizes, utils.PromptType.BBOX\n",
    "        )\n",
    "points, flag_points = utils.annotations_to_tensor(\n",
    "            prompts_processor, points, img_sizes, utils.PromptType.POINT\n",
    "        )\n",
    "masks, flag_masks = utils.annotations_to_tensor(\n",
    "            prompts_processor, masks, img_sizes, utils.PromptType.MASK\n",
    "        )\n",
    "\n",
    "# Flag examples are used to tell the model each examples which class has (it is a merge of flag_bboxes, flag_masks, flag_points)\n",
    "flag_examples = utils.flags_merge(flag_bboxes=flag_bboxes, flag_points=flag_points, flag_masks=flag_masks)\n",
    "\n",
    "print(bboxes) # Size is (num_support_images, num_classes, num_bboxes, 4)\n",
    "print(flag_bboxes) # Size is (num_support_images, num_classes)\n",
    "print(points) # Size is (num_support_images, num_classes, num_points, 2)\n",
    "print(flag_points) # Size is (num_support_images, num_classes)\n",
    "print(masks) # Size is (num_support_images, num_classes, 256, 256)\n",
    "print(flag_masks) # Size is (num_support_images, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "drawn_images = [\n",
    "    draw_all(get_image(image), img_masks, img_bboxes, img_points, colors)\n",
    "    for image, img_masks, img_bboxes, img_points in zip(\n",
    "        support_images, masks, bboxes, points\n",
    "    )\n",
    "]\n",
    "drawn_images[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_dict = {\n",
    "    utils.BatchKeys.IMAGES: torch.stack([query_image] + support_images).unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_BBOXES: bboxes.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_BBOXES: flag_bboxes.unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_POINTS: points.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_POINTS: flag_points.unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_MASKS: masks.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_MASKS: flag_masks.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_EXAMPLES: flag_examples.unsqueeze(0),\n",
    "    utils.BatchKeys.DIMS: torch.tensor([img_sizes]),\n",
    "}\n",
    "input_dict = dict_to_device(input_dict, accelerator.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = la(input_dict)\n",
    "logits = output[\"logits\"]\n",
    "\n",
    "predictions = torch.argmax(logits, dim=1)\n",
    "predictions.chans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf58c475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79029683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644efa2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d00dc345",
   "metadata": {},
   "source": [
    "# マスクだけ画像で与える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d4c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "sys.path.append(str(Path.cwd().parent / 'label_anything'))\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Pretty visualizations of tensors\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "from lovely_numpy import lo\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from label_anything import LabelAnything\n",
    "from label_anything.data import get_preprocessing, utils\n",
    "from label_anything.data.transforms import PromptsProcessor\n",
    "\n",
    "# -----------------------------\n",
    "# 可視化ユーティリティ\n",
    "# -----------------------------\n",
    "colors = [\n",
    "    (255, 255, 0),   # yellow\n",
    "    (255, 0, 0),     # red\n",
    "    (0, 255, 0),     # green\n",
    "    (0, 0, 255),     # blue\n",
    "    (255, 0, 255),   # purple\n",
    "    (0, 255, 255),   # cyan\n",
    "    (255, 165, 0),   # orange\n",
    "    (255, 192, 203), # pink\n",
    "    (139, 69, 19),   # brown\n",
    "    (128, 128, 128), # grey\n",
    "    (0, 0, 0),       # black\n",
    "] * 3\n",
    "\n",
    "def draw_masks(img: Image.Image, masks: torch.Tensor, colors):\n",
    "    masked_image = resize(img.copy(), 256)\n",
    "    for i, mask in enumerate(masks):\n",
    "        mask = mask.numpy()\n",
    "        masked_image = np.where(np.repeat(mask[:, :, np.newaxis], 3, axis=2),\n",
    "                                np.asarray(colors[i], dtype=\"uint8\"),\n",
    "                                masked_image)\n",
    "    masked_image = masked_image.astype(np.uint8)\n",
    "    return cv2.addWeighted(np.array(resize(img, 256)), 0.3, masked_image, 0.7, 0)\n",
    "\n",
    "def draw_boxes(img: Image.Image, boxes: torch.Tensor, colors):\n",
    "    img = np.array(img)\n",
    "    for i, cat in enumerate(boxes):\n",
    "        for box in cat:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            img = cv2.rectangle(img, (x1, y1), (x2, y2), colors[i], 2)\n",
    "    return img\n",
    "\n",
    "def draw_points(img: Image.Image, points: torch.Tensor, colors):\n",
    "    img = np.array(img)\n",
    "    for i, cat in enumerate(points):\n",
    "        for point in cat:\n",
    "            x, y = map(int, point)\n",
    "            img = cv2.circle(img, (x, y), 5, colors[i], -1)\n",
    "    return img\n",
    "\n",
    "def draw_all(img: Image.Image, masks, boxes, points, colors):\n",
    "    segmented_image = draw_masks(img, masks, colors)\n",
    "    img = Image.fromarray(segmented_image)\n",
    "    img = resize(img, 1024)\n",
    "    img = draw_boxes(img, boxes, colors)\n",
    "    img = Image.fromarray(img)\n",
    "    img = draw_points(img, points, colors)\n",
    "    img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "def get_image(image_tensor: torch.Tensor) -> Image.Image:\n",
    "    MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "    STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "    unnormalized_image = (image_tensor.numpy() * np.array(STD)[:, None, None]) + np.array(MEAN)[:, None, None]\n",
    "    unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "    return Image.fromarray(unnormalized_image)\n",
    "\n",
    "def dict_to_device(d, device):\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            d[k] = v.to(device)\n",
    "        elif isinstance(v, dict):\n",
    "            d[k] = dict_to_device(v, device)\n",
    "        elif isinstance(v, list):\n",
    "            d[k] = [dict_to_device(vv, device) for vv in v]\n",
    "    return d\n",
    "\n",
    "# -----------------------------\n",
    "# 新規: アノテーション読み込み関連\n",
    "# -----------------------------\n",
    "ANNOT_DIR = Path.cwd() / \"annotations\"\n",
    "CLASS_NAME_TO_ID = {\n",
    "    \"handrail\": 1,\n",
    "    \"midrail\": 2,\n",
    "    \"toeboard\": 3,\n",
    "}\n",
    "\n",
    "def _parse_class_key(k: Any, class_map: Dict[str, int]) -> int:\n",
    "    \"\"\"\n",
    "    JSON のクラスキーを int に正規化。\n",
    "    - 数字文字列や数値は int に\n",
    "    - 文字列で handrail/midrail/toeboard は対応 ID に\n",
    "    \"\"\"\n",
    "    if isinstance(k, int):\n",
    "        return k\n",
    "    if isinstance(k, str):\n",
    "        ks = k.strip().lower()\n",
    "        if ks.isdigit():\n",
    "            return int(ks)\n",
    "        if ks in class_map:\n",
    "            return class_map[ks]\n",
    "    raise ValueError(f\"Unsupported class key: {k}\")\n",
    "\n",
    "def load_annotations_for_image(stem: str) -> Dict[str, Dict[int, list]]:\n",
    "    \"\"\"\n",
    "    annotations/<stem>.json を読み込み、{ 'bboxes','points','masks' } を\n",
    "    {class_id: list} 形式で返す。無ければ空辞書。\n",
    "    \"\"\"\n",
    "    ann_path = ANNOT_DIR / f\"{stem}.json\"\n",
    "    result = {\"bboxes\": {}, \"points\": {}, \"masks\": {}}\n",
    "    if not ann_path.exists():\n",
    "        return result\n",
    "\n",
    "    with open(ann_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # classes があればマージ（優先: ファイル）\n",
    "    class_map = dict(CLASS_NAME_TO_ID)\n",
    "    if \"classes\" in data and isinstance(data[\"classes\"], dict):\n",
    "        for name, cid in data[\"classes\"].items():\n",
    "            try:\n",
    "                class_map[str(name).strip().lower()] = int(cid)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # bboxes / points / masks をクラスIDキーに正規化\n",
    "    for k in [\"bboxes\", \"points\", \"masks\"]:\n",
    "        raw = data.get(k, {})\n",
    "        if isinstance(raw, dict):\n",
    "            for kk, vv in raw.items():\n",
    "                cid = _parse_class_key(kk, class_map)\n",
    "                if not isinstance(vv, list):\n",
    "                    vv = []\n",
    "                result[k][cid] = vv\n",
    "    return result\n",
    "\n",
    "def union_class_ids(dicts_per_support: List[Dict[str, Dict[int, list]]]) -> List[int]:\n",
    "    s = set()\n",
    "    for d in dicts_per_support:\n",
    "        for k in [\"bboxes\", \"points\", \"masks\"]:\n",
    "            s |= set(d.get(k, {}).keys())\n",
    "    return sorted(list(s))\n",
    "\n",
    "# -----------------------------\n",
    "# モデル/前処理 準備\n",
    "# -----------------------------\n",
    "image_size = 1024\n",
    "custom_preprocess = True\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "# Label Anything 本体\n",
    "la = LabelAnything.from_pretrained(\"pasqualedem/label_anything_sam_1024_coco\")\n",
    "\n",
    "# 画像ロード\n",
    "img_dir = Path.cwd() / \"images\"\n",
    "img_paths = sorted(list(img_dir.glob(\"*.jpg\")) + list(img_dir.glob(\"*.jpeg\")) +\n",
    "                   list(img_dir.glob(\"*.png\")) + list(img_dir.glob(\"*.JPG\")) + list(img_dir.glob(\"*.PNG\")))\n",
    "assert len(img_paths) >= 2, \"画像はクエリ1 + サポート1以上が必要です（./images に配置）。\"\n",
    "\n",
    "def open_rgb(p: Path) -> Image.Image:\n",
    "    return Image.open(p).convert(\"RGB\")\n",
    "\n",
    "# 先頭=クエリ、残り=サポート\n",
    "query_orig = open_rgb(img_paths[0])\n",
    "support_orig_images = [open_rgb(p) for p in img_paths[1:]]\n",
    "\n",
    "# 前処理（LA の transform）\n",
    "preprocess = get_preprocessing(\n",
    "    {\n",
    "        \"common\": {\n",
    "            \"custom_preprocess\": custom_preprocess,\n",
    "            \"image_size\": image_size,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "query_image = preprocess(query_orig)\n",
    "support_images = [preprocess(img) for img in support_orig_images]\n",
    "\n",
    "# 画像サイズ（LA内部の tensor 化/正規化用）\n",
    "support_sizes: List[Tuple[int, int]] = [img.size for img in support_orig_images]   # (W,H)\n",
    "all_sizes: List[Tuple[int, int]] = [query_orig.size] + support_sizes               # DIMS 用\n",
    "\n",
    "# プロンプト前処理器\n",
    "prompts_processor = PromptsProcessor(\n",
    "    long_side_length=image_size,\n",
    "    masks_side_length=256,\n",
    "    custom_preprocess=custom_preprocess,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 新規: JSON から bbox/point/mask を構築\n",
    "# -----------------------------\n",
    "# 1) 各サポート画像について annotations/<stem>.json を読む\n",
    "support_annots: List[Dict[str, Dict[int, list]]] = []\n",
    "for p in img_paths[1:]:\n",
    "    support_annots.append(load_annotations_for_image(p.stem))\n",
    "\n",
    "# 2) 全サポートで現れるクラスIDの集合を抽出（背景 -1 は後で付与）\n",
    "cat_ids = union_class_ids(support_annots)\n",
    "if not cat_ids:\n",
    "    # どれも空なら、念のため既定の3クラスを用意（handrail, midrail, toeboard）\n",
    "    cat_ids = [1, 2, 3]\n",
    "cat_ids = sorted(cat_ids)\n",
    "\n",
    "# 3) bboxes/points/masks を Label Anything 期待の形に揃える\n",
    "#    形: List[ support画像ごとに { class_id: List[...] } ]\n",
    "bboxes_list: List[Dict[int, List[List[float]]]] = []\n",
    "points_list: List[Dict[int, List[List[float]]]] = []\n",
    "masks_list: List[Dict[int, List[np.ndarray]]] = []\n",
    "\n",
    "for idx, ann in enumerate(support_annots):\n",
    "    per_img_b = {}\n",
    "    per_img_p = {}\n",
    "    per_img_m = {}\n",
    "\n",
    "    # すべてのクラスについてキーを保証\n",
    "    for cid in cat_ids:\n",
    "        per_img_b[cid] = list(ann.get(\"bboxes\", {}).get(cid, []))\n",
    "        per_img_p[cid] = list(ann.get(\"points\", {}).get(cid, []))\n",
    "        # マスクは画像として読み込む\n",
    "        mask_paths = ann.get(\"masks\", {}).get(cid, [])\n",
    "        masks_np = []\n",
    "        for mp in mask_paths:\n",
    "            mp_abs = (ANNOT_DIR / mp).resolve()\n",
    "            if mp_abs.exists():\n",
    "                mimg = Image.open(mp_abs).convert(\"L\")   # グレイスケールでOK（0/255 or index）\n",
    "                masks_np.append(np.array(mimg))\n",
    "        per_img_m[cid] = masks_np\n",
    "\n",
    "    bboxes_list.append(per_img_b)\n",
    "    points_list.append(per_img_p)\n",
    "    masks_list.append(per_img_m)\n",
    "\n",
    "# 4) bbox は LA 形式に変換（正規化など）\n",
    "#    convert_bbox は (x1,y1,x2,y2, *image.size) を前提\n",
    "converted_bboxes: List[Dict[int, List[List[float]]]] = []\n",
    "for img_bboxes, orig_img in zip(bboxes_list, support_orig_images):\n",
    "    out = {}\n",
    "    for cid, cat_bboxes in img_bboxes.items():\n",
    "        out[cid] = [\n",
    "            prompts_processor.convert_bbox(bbox, *orig_img.size, noise=False)\n",
    "            for bbox in cat_bboxes\n",
    "        ]\n",
    "    converted_bboxes.append(out)\n",
    "\n",
    "# 5) 背景 -1 を追加（空）\n",
    "bboxes_list_bg = [{**{-1: []}, **bb} for bb in converted_bboxes]\n",
    "points_list_bg = [{**{-1: []}, **pp} for pp in points_list]\n",
    "masks_list_bg  = [{**{-1: []}, **mm} for mm in masks_list]\n",
    "cat_ids_with_bg = [-1] + cat_ids\n",
    "\n",
    "# 6) numpy 配列化（クラスごとに空でも配列化）\n",
    "for i in range(len(bboxes_list_bg)):\n",
    "    for cid in cat_ids_with_bg:\n",
    "        bboxes_list_bg[i][cid] = np.array(bboxes_list_bg[i][cid], dtype=np.float32)\n",
    "        points_list_bg[i][cid] = np.array(points_list_bg[i][cid], dtype=np.float32)\n",
    "        # masks はすでに np.ndarray のリスト（LA 側で 256 へ整形）\n",
    "\n",
    "# 7) tensor 化（スタック＆パディング）※ img_sizes は「サポート画像」サイズを渡す\n",
    "bboxes, flag_bboxes = utils.annotations_to_tensor(\n",
    "    prompts_processor, bboxes_list_bg, support_sizes, utils.PromptType.BBOX\n",
    ")\n",
    "points, flag_points = utils.annotations_to_tensor(\n",
    "    prompts_processor, points_list_bg, support_sizes, utils.PromptType.POINT\n",
    ")\n",
    "masks,  flag_masks  = utils.annotations_to_tensor(\n",
    "    prompts_processor, masks_list_bg,  support_sizes, utils.PromptType.MASK\n",
    ")\n",
    "\n",
    "# 各クラスが存在するかの統合フラグ\n",
    "flag_examples = utils.flags_merge(\n",
    "    flag_bboxes=flag_bboxes, flag_points=flag_points, flag_masks=flag_masks\n",
    ")\n",
    "\n",
    "print(\"bboxes:\", bboxes.shape)       # (num_support, num_classes+1, num_bboxes, 4)\n",
    "print(\"points:\", points.shape)       # (num_support, num_classes+1, num_points, 2)\n",
    "print(\"masks:\", masks.shape)         # (num_support, num_classes+1, 256, 256)\n",
    "\n",
    "# -----------------------------\n",
    "# 可視化（任意）\n",
    "# -----------------------------\n",
    "drawn_images = [\n",
    "    draw_all(get_image(img_t), img_masks, img_bboxes, img_points, colors)\n",
    "    for img_t, img_masks, img_bboxes, img_points in zip(\n",
    "        support_images, masks, bboxes, points\n",
    "    )\n",
    "]\n",
    "# 例: 最初のサポートを表示用に確認（環境に応じて）\n",
    "# Image.fromarray(drawn_images[0]).show()\n",
    "\n",
    "# -----------------------------\n",
    "# 推論\n",
    "# -----------------------------\n",
    "input_dict = {\n",
    "    utils.BatchKeys.IMAGES: torch.stack([query_image] + support_images).unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_BBOXES: bboxes.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_BBOXES:   flag_bboxes.unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_POINTS: points.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_POINTS:   flag_points.unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_MASKS:  masks.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_MASKS:    flag_masks.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_EXAMPLES: flag_examples.unsqueeze(0),\n",
    "    # DIMS は [query] + [supports] の順に (W,H)\n",
    "    utils.BatchKeys.DIMS: torch.tensor([all_sizes], dtype=torch.int32),\n",
    "}\n",
    "input_dict = dict_to_device(input_dict, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = la(input_dict)\n",
    "logits = output[\"logits\"]\n",
    "predictions = torch.argmax(logits, dim=1)\n",
    "predictions.chans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122ae7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad28fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee0de16",
   "metadata": {},
   "source": [
    "# マスクも含めて “すべて JSON” から読む版のフルコードです。\n",
    "マスクは JSON 内で RLE（COCO形式）または polygons で与えられる想定にしました（PNG等の外部画像は使いません）。RLE を使う場合は pycocotools が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee64caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bboxes: torch.Size([1, 4, 0, 4])\n",
      "points: torch.Size([1, 4, 0, 2])\n",
      "masks: torch.Size([1, 4, 256, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 408\u001b[39m\n\u001b[32m    405\u001b[39m input_dict = dict_to_device(input_dict, device)\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m     output = \u001b[43mla\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    409\u001b[39m logits = output[\u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    410\u001b[39m predictions = torch.argmax(logits, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/label_anything/models/build_lam.py:508\u001b[39m, in \u001b[36mLabelAnything.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/label_anything/models/lam.py:90\u001b[39m, in \u001b[36mLam.forward\u001b[39m\u001b[34m(self, batched_input)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mself\u001b[39m, batched_input: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]\n\u001b[32m     59\u001b[39m ) -> List[Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]]:\n\u001b[32m     60\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m    Predicts masks end-to-end from provided images and prompts.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m    If prompts are not known in advance, using SamPredictor is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     88\u001b[39m \u001b[33;03m            original size of the image.\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     seg, pe_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     seg = \u001b[38;5;28mself\u001b[39m.postprocess_masks(seg, batched_input[\u001b[33m\"\u001b[39m\u001b[33mdims\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mflag_gts\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_input:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/label_anything/models/lam.py:116\u001b[39m, in \u001b[36mLam._forward\u001b[39m\u001b[34m(self, batched_input)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batched_input: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     query_embeddings, prompt_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_query_example_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     points, boxes, masks, flag_examples = \u001b[38;5;28mself\u001b[39m.prepare_prompts(batched_input)\n\u001b[32m    121\u001b[39m     pe_result = \u001b[38;5;28mself\u001b[39m.prompt_encoder(\n\u001b[32m    122\u001b[39m         image_embeddings=prompt_embeddings,\n\u001b[32m    123\u001b[39m         points=points,\n\u001b[32m   (...)\u001b[39m\u001b[32m    126\u001b[39m         flag_examples=flag_examples,\n\u001b[32m    127\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/label_anything/models/lam.py:160\u001b[39m, in \u001b[36mLam.prepare_query_example_embeddings\u001b[39m\u001b[34m(self, batched_input)\u001b[39m\n\u001b[32m    158\u001b[39m B, N, C, H, W = batched_input[\u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m].shape\n\u001b[32m    159\u001b[39m images = rearrange(batched_input[\u001b[33m\"\u001b[39m\u001b[33mimages\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mb n c h w -> (b n) c h w\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.neck \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    162\u001b[39m     embeddings = \u001b[38;5;28mself\u001b[39m.neck(embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/label_anything/models/image_encoder.py:112\u001b[39m, in \u001b[36mImageEncoderViT.forward\u001b[39m\u001b[34m(self, x, return_last_block_state)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, return_last_block_state: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> torch.Tensor:\n\u001b[32m    111\u001b[39m     \u001b[38;5;66;03m# return torch.rand(5, 768, 64, 64).cuda()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpatch_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    114\u001b[39m         x = x + \u001b[38;5;28mself\u001b[39m.pos_embed\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/label_anything/models/image_encoder.py:407\u001b[39m, in \u001b[36mPatchEmbed.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m     \u001b[38;5;66;03m# B C H W -> B H W C\u001b[39;00m\n\u001b[32m    409\u001b[39m     x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "sys.path.append(str(Path.cwd().parent / 'label_anything'))\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Pretty visualizations of tensors\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "from lovely_numpy import lo\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from label_anything import LabelAnything\n",
    "from label_anything.data import get_preprocessing, utils\n",
    "from label_anything.data.transforms import PromptsProcessor\n",
    "\n",
    "# -----------------------------\n",
    "# 可視化ユーティリティ\n",
    "# -----------------------------\n",
    "colors = [\n",
    "    (255, 255, 0),   # yellow\n",
    "    (255, 0, 0),     # red\n",
    "    (0, 255, 0),     # green\n",
    "    (0, 0, 255),     # blue\n",
    "    (255, 0, 255),   # purple\n",
    "    (0, 255, 255),   # cyan\n",
    "    (255, 165, 0),   # orange\n",
    "    (255, 192, 203), # pink\n",
    "    (139, 69, 19),   # brown\n",
    "    (128, 128, 128), # grey\n",
    "    (0, 0, 0),       # black\n",
    "] * 3\n",
    "\n",
    "def draw_masks(img: Image.Image, masks: torch.Tensor, colors):\n",
    "    masked_image = resize(img.copy(), 256)\n",
    "    for i, mask in enumerate(masks):\n",
    "        mask = mask.numpy()\n",
    "        masked_image = np.where(np.repeat(mask[:, :, np.newaxis], 3, axis=2),\n",
    "                                np.asarray(colors[i], dtype=\"uint8\"),\n",
    "                                masked_image)\n",
    "    masked_image = masked_image.astype(np.uint8)\n",
    "    return cv2.addWeighted(np.array(resize(img, 256)), 0.3, masked_image, 0.7, 0)\n",
    "\n",
    "def draw_boxes(img: Image.Image, boxes: torch.Tensor, colors):\n",
    "    img = np.array(img)\n",
    "    for i, cat in enumerate(boxes):\n",
    "        for box in cat:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            img = cv2.rectangle(img, (x1, y1), (x2, y2), colors[i], 2)\n",
    "    return img\n",
    "\n",
    "def draw_points(img: Image.Image, points: torch.Tensor, colors):\n",
    "    img = np.array(img)\n",
    "    for i, cat in enumerate(points):\n",
    "        for point in cat:\n",
    "            x, y = map(int, point)\n",
    "            img = cv2.circle(img, (x, y), 5, colors[i], -1)\n",
    "    return img\n",
    "\n",
    "def draw_all(img: Image.Image, masks, boxes, points, colors):\n",
    "    segmented_image = draw_masks(img, masks, colors)\n",
    "    img = Image.fromarray(segmented_image)\n",
    "    img = resize(img, 1024)\n",
    "    img = draw_boxes(img, boxes, colors)\n",
    "    img = Image.fromarray(img)\n",
    "    img = draw_points(img, points, colors)\n",
    "    img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "def get_image(image_tensor: torch.Tensor) -> Image.Image:\n",
    "    MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "    STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "    unnormalized_image = (image_tensor.numpy() * np.array(STD)[:, None, None]) + np.array(MEAN)[:, None, None]\n",
    "    unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "    return Image.fromarray(unnormalized_image)\n",
    "\n",
    "def dict_to_device(d, device):\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            d[k] = v.to(device)\n",
    "        elif isinstance(v, dict):\n",
    "            d[k] = dict_to_device(v, device)\n",
    "        elif isinstance(v, list):\n",
    "            d[k] = [dict_to_device(vv, device) for vv in v]\n",
    "    return d\n",
    "\n",
    "# -----------------------------\n",
    "# すべて JSON 読み込み関連（RLE/Polygonをサポート）\n",
    "# -----------------------------\n",
    "ANNOT_DIR = Path.cwd() / \"annotations\"\n",
    "CLASS_NAME_TO_ID = {\n",
    "    \"handrail\": 1,\n",
    "    \"midrail\": 2,\n",
    "    \"baseboard\": 3,\n",
    "    # 日本語名を使うならここに追記可\n",
    "    \"手すり\": 1,\n",
    "    \"中桟\": 2,\n",
    "    \"巾木\": 3,\n",
    "}\n",
    "\n",
    "try:\n",
    "    from pycocotools import mask as mask_utils\n",
    "except Exception:\n",
    "    mask_utils = None\n",
    "\n",
    "def _parse_class_key(k: Any, class_map: Dict[str, int]) -> int:\n",
    "    \"\"\"\n",
    "    JSON のクラスキーを int に正規化。\n",
    "    - 数字文字列や数値は int に\n",
    "    - 文字列で handrail/midrail/toeboard（または classes で定義）なら対応 ID に\n",
    "    \"\"\"\n",
    "    if isinstance(k, int):\n",
    "        return k\n",
    "    if isinstance(k, str):\n",
    "        ks = k.strip().lower()\n",
    "        if ks.isdigit():\n",
    "            return int(ks)\n",
    "        if ks in class_map:\n",
    "            return class_map[ks]\n",
    "    raise ValueError(f\"Unsupported class key: {k}\")\n",
    "\n",
    "def _mask_from_polygons(size_hw: List[int], polygons: List[List[float]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    polygons: [[x1,y1,x2,y2,...], ...]\n",
    "    size_hw: [H, W]\n",
    "    return: 0/255 の 2値マスク (H, W)\n",
    "    \"\"\"\n",
    "    H, W = int(size_hw[0]), int(size_hw[1])\n",
    "    m = np.zeros((H, W), dtype=np.uint8)\n",
    "    for poly in polygons:\n",
    "        if not poly:\n",
    "            continue\n",
    "        pts = np.array(poly, dtype=np.float32).reshape(-1, 2)\n",
    "        pts = np.round(pts).astype(np.int32)\n",
    "        cv2.fillPoly(m, [pts], 255)\n",
    "    return m\n",
    "\n",
    "def _mask_from_rle(size_hw: List[int], counts) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    COCO RLE をデコードして 0/255 の 2値マスクに。\n",
    "    counts: 圧縮文字列 or 非圧縮リスト（どちらも COCO 互換）\n",
    "    pycocotools が無ければ RLE は使えない（polygons を使ってください）。\n",
    "    \"\"\"\n",
    "    if mask_utils is None:\n",
    "        raise RuntimeError(\n",
    "            \"RLE マスクを使うには pycocotools が必要です。`pip install pycocotools` をインストールするか、\"\n",
    "            \"JSON の masks を polygons 形式にしてください。\"\n",
    "        )\n",
    "    rle = {\"size\": [int(size_hw[0]), int(size_hw[1])], \"counts\": counts}\n",
    "    m = mask_utils.decode(rle)  # (H,W) or (H,W,1)\n",
    "    if m.ndim == 3:\n",
    "        m = m[..., 0]\n",
    "    return (m.astype(np.uint8) * 255)\n",
    "\n",
    "def _load_mask_entry_json(entry: Any) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    entry:\n",
    "      - {\"format\":\"rle\", \"size\":[H,W], \"counts\":\"<COCO compressed string>\"}\n",
    "      - {\"format\":\"rle\", \"size\":[H,W], \"counts\":[...]}  # 非圧縮 RLE リスト\n",
    "      - {\"format\":\"polygons\", \"size\":[H,W], \"polygons\":[[x1,y1,...], ...]}\n",
    "    return: 0/255 の 2値マスク (H, W)\n",
    "    \"\"\"\n",
    "    if not isinstance(entry, dict):\n",
    "        raise ValueError(f\"Mask entry must be dict, got {type(entry)}\")\n",
    "\n",
    "    fmt = str(entry.get(\"format\", \"\")).lower()\n",
    "    size = entry.get(\"size\", None)\n",
    "    if not (isinstance(size, (list, tuple)) and len(size) == 2):\n",
    "        raise ValueError(f\"Mask entry missing valid 'size': {entry}\")\n",
    "\n",
    "    if fmt == \"polygons\":\n",
    "        polygons = entry.get(\"polygons\", [])\n",
    "        return _mask_from_polygons(size, polygons)\n",
    "\n",
    "    if fmt == \"rle\":\n",
    "        counts = entry.get(\"counts\", None)\n",
    "        if counts is None:\n",
    "            raise ValueError(f\"RLE entry missing 'counts': {entry}\")\n",
    "        return _mask_from_rle(size, counts)\n",
    "\n",
    "    raise ValueError(f\"Unsupported mask format: {fmt}\")\n",
    "\n",
    "def load_annotations_for_image(stem: str) -> Dict[str, Dict[int, list]]:\n",
    "    \"\"\"\n",
    "    annotations/<stem>.json を読み込み、\n",
    "      - 'bboxes' : {class_id: [[x1,y1,x2,y2], ...]}\n",
    "      - 'points' : {class_id: [[x,y], ...]}\n",
    "      - 'masks'  : {class_id: [np.ndarray(H,W;0/255), ...]}  ← JSON 内の RLE/Polygon を 2値マスクに変換\n",
    "    の形で返す。ファイルが無ければ空辞書を返す。\n",
    "    \"\"\"\n",
    "    ann_path = ANNOT_DIR / f\"{stem}.json\"\n",
    "    result = {\"bboxes\": {}, \"points\": {}, \"masks\": {}}\n",
    "    if not ann_path.exists():\n",
    "        return result\n",
    "\n",
    "    with open(ann_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # classes があればマージ（優先: ファイル）\n",
    "    class_map = dict(CLASS_NAME_TO_ID)\n",
    "    if \"classes\" in data and isinstance(data[\"classes\"], dict):\n",
    "        for name, cid in data[\"classes\"].items():\n",
    "            try:\n",
    "                class_map[str(name).strip().lower()] = int(cid)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # bboxes / points をクラスIDキーに正規化\n",
    "    for k in [\"bboxes\", \"points\"]:\n",
    "        raw = data.get(k, {})\n",
    "        if isinstance(raw, dict):\n",
    "            for kk, vv in raw.items():\n",
    "                cid = _parse_class_key(kk, class_map)\n",
    "                if not isinstance(vv, list):\n",
    "                    vv = []\n",
    "                result[k][cid] = vv\n",
    "\n",
    "    # masks: JSON内の各エントリを実マスク(np.ndarray)に変換\n",
    "    raw_masks = data.get(\"masks\", {})\n",
    "    if isinstance(raw_masks, dict):\n",
    "        for kk, entries in raw_masks.items():\n",
    "            cid = _parse_class_key(kk, class_map)\n",
    "            masks_np = []\n",
    "            if isinstance(entries, list):\n",
    "                for entry in entries:\n",
    "                    m = _load_mask_entry_json(entry)\n",
    "                    if m is not None:\n",
    "                        masks_np.append(m.astype(np.uint8))\n",
    "            result[\"masks\"][cid] = masks_np\n",
    "\n",
    "    return result\n",
    "\n",
    "def union_class_ids(dicts_per_support: List[Dict[str, Dict[int, list]]]) -> List[int]:\n",
    "    s = set()\n",
    "    for d in dicts_per_support:\n",
    "        for k in [\"bboxes\", \"points\", \"masks\"]:\n",
    "            s |= set(d.get(k, {}).keys())\n",
    "    return sorted(list(s))\n",
    "\n",
    "# -----------------------------\n",
    "# モデル/前処理 準備\n",
    "# -----------------------------\n",
    "image_size = 1024\n",
    "custom_preprocess = True\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "\n",
    "# Label Anything 本体\n",
    "la = LabelAnything.from_pretrained(\"pasqualedem/label_anything_sam_1024_coco\")\n",
    "\n",
    "# 画像ロード\n",
    "img_dir = Path.cwd() / \"images\"\n",
    "img_paths = sorted(list(img_dir.glob(\"*.jpg\")) + list(img_dir.glob(\"*.jpeg\")) +\n",
    "                   list(img_dir.glob(\"*.png\")) + list(img_dir.glob(\"*.JPG\")) + list(img_dir.glob(\"*.PNG\")))\n",
    "assert len(img_paths) >= 2, \"画像はクエリ1 + サポート1以上が必要です（./images に配置）。\"\n",
    "\n",
    "def open_rgb(p: Path) -> Image.Image:\n",
    "    return Image.open(p).convert(\"RGB\")\n",
    "\n",
    "# 先頭=クエリ、残り=サポート\n",
    "query_orig = open_rgb(img_paths[0])\n",
    "support_orig_images = [open_rgb(p) for p in img_paths[1:]]\n",
    "\n",
    "# 前処理（LA の transform）\n",
    "preprocess = get_preprocessing(\n",
    "    {\n",
    "        \"common\": {\n",
    "            \"custom_preprocess\": custom_preprocess,\n",
    "            \"image_size\": image_size,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "query_image = preprocess(query_orig)\n",
    "support_images = [preprocess(img) for img in support_orig_images]\n",
    "\n",
    "# 画像サイズ（LA内部の tensor 化/正規化用）\n",
    "support_sizes: List[Tuple[int, int]] = [img.size for img in support_orig_images]   # (W,H)\n",
    "all_sizes: List[Tuple[int, int]] = [query_orig.size] + support_sizes               # DIMS 用\n",
    "\n",
    "# プロンプト前処理器\n",
    "prompts_processor = PromptsProcessor(\n",
    "    long_side_length=image_size,\n",
    "    masks_side_length=256,\n",
    "    custom_preprocess=custom_preprocess,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# JSON から bbox/point/mask を構築（すべて JSON）\n",
    "# -----------------------------\n",
    "# 1) 各サポート画像について annotations/<stem>.json を読む\n",
    "support_annots: List[Dict[str, Dict[int, list]]] = []\n",
    "for p in img_paths[1:]:\n",
    "    support_annots.append(load_annotations_for_image(p.stem))\n",
    "\n",
    "# 2) 全サポートで現れるクラスIDの集合を抽出（背景 -1 は後で付与）\n",
    "cat_ids = union_class_ids(support_annots)\n",
    "if not cat_ids:\n",
    "    # どれも空なら、念のため既定の3クラスを用意（handrail, midrail, toeboard）\n",
    "    cat_ids = [1, 2, 3]\n",
    "cat_ids = sorted(cat_ids)\n",
    "\n",
    "# 3) bboxes/points/masks を Label Anything 期待の形に揃える\n",
    "#    形: List[ support画像ごとに { class_id: List[...] } ]\n",
    "bboxes_list: List[Dict[int, List[List[float]]]] = []\n",
    "points_list: List[Dict[int, List[List[float]]]] = []\n",
    "masks_list:  List[Dict[int, List[np.ndarray]]] = []\n",
    "\n",
    "for ann in support_annots:\n",
    "    per_img_b = {}\n",
    "    per_img_p = {}\n",
    "    per_img_m = {}\n",
    "\n",
    "    # すべてのクラスについてキーを保証\n",
    "    for cid in cat_ids:\n",
    "        per_img_b[cid] = list(ann.get(\"bboxes\", {}).get(cid, []))\n",
    "        per_img_p[cid] = list(ann.get(\"points\", {}).get(cid, []))\n",
    "        per_img_m[cid] = list(ann.get(\"masks\",  {}).get(cid, []))  # ここはすでに np.ndarray のリスト\n",
    "\n",
    "    bboxes_list.append(per_img_b)\n",
    "    points_list.append(per_img_p)\n",
    "    masks_list.append(per_img_m)\n",
    "\n",
    "# 4) bbox は LA 形式に変換（正規化など）\n",
    "#    convert_bbox は (x1,y1,x2,y2, *image.size) を前提\n",
    "converted_bboxes: List[Dict[int, List[List[float]]]] = []\n",
    "for img_bboxes, orig_img in zip(bboxes_list, support_orig_images):\n",
    "    out = {}\n",
    "    for cid, cat_bboxes in img_bboxes.items():\n",
    "        out[cid] = [\n",
    "            prompts_processor.convert_bbox(bbox, *orig_img.size, noise=False)\n",
    "            for bbox in cat_bboxes\n",
    "        ]\n",
    "    converted_bboxes.append(out)\n",
    "\n",
    "# 5) 背景 -1 を追加（空）\n",
    "bboxes_list_bg = [{**{-1: []}, **bb} for bb in converted_bboxes]\n",
    "points_list_bg = [{**{-1: []}, **pp} for pp in points_list]\n",
    "masks_list_bg  = [{**{-1: []}, **mm} for mm in masks_list]\n",
    "cat_ids_with_bg = [-1] + cat_ids\n",
    "\n",
    "# 6) numpy 配列化（クラスごとに空でも配列化）\n",
    "for i in range(len(bboxes_list_bg)):\n",
    "    for cid in cat_ids_with_bg:\n",
    "        bboxes_list_bg[i][cid] = np.array(bboxes_list_bg[i][cid], dtype=np.float32)\n",
    "        points_list_bg[i][cid] = np.array(points_list_bg[i][cid], dtype=np.float32)\n",
    "        # masks はすでに np.ndarray のリスト（LA 側で 256 へ整形）\n",
    "\n",
    "# 7) tensor 化（スタック＆パディング）※ img_sizes は「サポート画像」サイズを渡す\n",
    "bboxes, flag_bboxes = utils.annotations_to_tensor(\n",
    "    prompts_processor, bboxes_list_bg, support_sizes, utils.PromptType.BBOX\n",
    ")\n",
    "points, flag_points = utils.annotations_to_tensor(\n",
    "    prompts_processor, points_list_bg, support_sizes, utils.PromptType.POINT\n",
    ")\n",
    "masks,  flag_masks  = utils.annotations_to_tensor(\n",
    "    prompts_processor, masks_list_bg,  support_sizes, utils.PromptType.MASK\n",
    ")\n",
    "\n",
    "# 各クラスが存在するかの統合フラグ\n",
    "flag_examples = utils.flags_merge(\n",
    "    flag_bboxes=flag_bboxes, flag_points=flag_points, flag_masks=flag_masks\n",
    ")\n",
    "\n",
    "print(\"bboxes:\", bboxes.shape)       # (num_support, num_classes+1, num_bboxes, 4)\n",
    "print(\"points:\", points.shape)       # (num_support, num_classes+1, num_points, 2)\n",
    "print(\"masks:\", masks.shape)         # (num_support, num_classes+1, 256, 256)\n",
    "\n",
    "# -----------------------------\n",
    "# 可視化（任意）\n",
    "# -----------------------------\n",
    "drawn_images = [\n",
    "    draw_all(get_image(img_t), img_masks, img_bboxes, img_points, colors)\n",
    "    for img_t, img_masks, img_bboxes, img_points in zip(\n",
    "        support_images, masks, bboxes, points\n",
    "    )\n",
    "]\n",
    "# Image.fromarray(drawn_images[0]).show()  # 必要なら\n",
    "\n",
    "# -----------------------------\n",
    "# 推論\n",
    "# -----------------------------\n",
    "input_dict = {\n",
    "    utils.BatchKeys.IMAGES: torch.stack([query_image] + support_images).unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_BBOXES: bboxes.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_BBOXES:   flag_bboxes.unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_POINTS: points.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_POINTS:   flag_points.unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_MASKS:  masks.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_MASKS:    flag_masks.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_EXAMPLES: flag_examples.unsqueeze(0),\n",
    "    # DIMS は [query] + [supports] の順に (W,H)\n",
    "    utils.BatchKeys.DIMS: torch.tensor([all_sizes], dtype=torch.int32),\n",
    "}\n",
    "input_dict = dict_to_device(input_dict, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = la(input_dict)\n",
    "logits = output[\"logits\"]\n",
    "predictions = torch.argmax(logits, dim=1)\n",
    "predictions.chans\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "label-anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
