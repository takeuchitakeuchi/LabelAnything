{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from pycocotools import mask as mask_utils\n",
    "import xml.etree.ElementTree as ET\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import binary_dilation, label\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni utili\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "sys.path.append(str(Path.cwd().parent / 'label_anything'))\n",
    "sys.path.append(str(Path.cwd().parent / 'label_anything' / 'data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [\n",
    "        {\"id\": 0, \"name\": \"bicycle\"},\n",
    "        {\"id\": 1, \"name\": \"horse\"},\n",
    "        {\"id\": 2, \"name\": \"motorbike\"},\n",
    "        {\"id\": 3, \"name\": \"diningtable\"},\n",
    "        {\"id\": 4, \"name\": \"cat\"},\n",
    "        {\"id\": 5, \"name\": \"aeroplane\"},\n",
    "        {\"id\": 6, \"name\": \"train\"},\n",
    "        {\"id\": 7, \"name\": \"dog\"},\n",
    "        {\"id\": 8, \"name\": \"tvmonitor\"},\n",
    "        {\"id\": 9, \"name\": \"bird\"},\n",
    "        {\"id\": 10, \"name\": \"sheep\"},\n",
    "        {\"id\": 11, \"name\": \"chair\"},\n",
    "        {\"id\": 12, \"name\": \"sofa\"},\n",
    "        {\"id\": 13, \"name\": \"person\"},\n",
    "        {\"id\": 14, \"name\": \"car\"},\n",
    "        {\"id\": 15, \"name\": \"bottle\"},\n",
    "        {\"id\": 16, \"name\": \"pottedplant\"},\n",
    "        {\"id\": 17, \"name\": \"bus\"},\n",
    "        {\"id\": 18, \"name\": \"cow\"},\n",
    "        {\"id\": 19, \"name\": \"boat\"},\n",
    "    ]\n",
    "\n",
    "def id_to_category(id, categories):\n",
    "    for category in categories:\n",
    "        if category['id'] == id:\n",
    "            return category['name']\n",
    "    return None\n",
    "\n",
    "a = id_to_category(1, categories)\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from pycocotools import mask as mask_utils\n",
    "from typing import Optional\n",
    "from PIL import Image\n",
    "from torch.nn.functional import one_hot\n",
    "import torch\n",
    "from label_anything.data.coco20i import Coco20iDataset\n",
    "from safetensors.torch import load_file\n",
    "from scipy.ndimage import binary_dilation, label\n",
    "\n",
    "from label_anything.data.utils import AnnFileKeys, BatchKeys, PromptType\n",
    "from label_anything.data.test import LabelAnythingTestDataset\n",
    "\n",
    "\n",
    "class VOC5i(Coco20iDataset):\n",
    "    def _load_safe(self, img_data: dict) -> (torch.Tensor, Optional[torch.Tensor]):  # type: ignore\n",
    "        \"\"\"Open a safetensors file and load the embedding and the ground truth.\n",
    "\n",
    "        Args:\n",
    "            img_data (dict): A dictionary containing the image data, as in the coco dataset.\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor, Optional[torch.Tensor]): Returns a tuple containing the embedding and the ground truth.\n",
    "        \"\"\"\n",
    "        assert self.emb_dir is not None, \"emb_dir must be provided.\"\n",
    "        gt = None\n",
    "\n",
    "        f = load_file(f\"{self.emb_dir}/{str(img_data[AnnFileKeys.ID])}.safetensors\")\n",
    "        embedding = f[\"embedding\"]\n",
    "        if self.load_gts:\n",
    "            gt = f[f\"{self.name}_gt\"]\n",
    "        return embedding, gt\n",
    "\n",
    "\n",
    "class PascalVOCTestDataset(LabelAnythingTestDataset):\n",
    "    num_classes = 20\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        img_dir: str,\n",
    "        annotations: str,\n",
    "        mask_dir_default: str,\n",
    "        mask_dir_aug: str,\n",
    "        preprocess=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.img_dir = img_dir  # data/raw/VOCdevkit/VOC2012/JPEGImages\n",
    "        self.annotations = annotations  # trainval_aug.txt\n",
    "        self.mask_dir = [\n",
    "            mask_dir_default,\n",
    "            mask_dir_aug,\n",
    "        ]  # segmentationClass, segmentationClassAug\n",
    "        self.preprocess = preprocess  # cose\n",
    "\n",
    "    def _read_image_ids(self):\n",
    "        ids = []\n",
    "        with open(self.annotations) as f:\n",
    "            for line in f:\n",
    "                image_path, _ = line.rstrip().split(\" \")\n",
    "                image_id = os.path.splitext(os.path.basename(image_path))[0]\n",
    "                ids.append(image_id)\n",
    "        return ids\n",
    "\n",
    "    def _get_label(self, image_id):\n",
    "        annotation_file = os.path.join(self.root, \"Annotations\", image_id + \".xml\")\n",
    "        objects = ET.parse(annotation_file).findall(\"object\")\n",
    "        labels = []\n",
    "\n",
    "        for object in objects:\n",
    "            class_name = object.find(\"name\").text.lower().strip()\n",
    "            labels.append(class_name)\n",
    "\n",
    "        return np.array(labels)\n",
    "\n",
    "    # def _image_to_category(self):\n",
    "    #     image_to_category = {}\n",
    "    #     for annotation in self.instances_path[\"annotations\"]:\n",
    "    #         image_id = annotation[\"image_id\"]\n",
    "    #         category_id = annotation[\"category_id\"]\n",
    "    #         image_to_category[image_id] = category_id\n",
    "    #     return image_to_category\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._read_image_ids())\n",
    "\n",
    "    def _get_image(self, image):\n",
    "        image_path = os.path.join(self.img_dir, image)\n",
    "        img = Image.open(image_path)\n",
    "        size = img.size\n",
    "        if self.preprocess:\n",
    "            img = self.preprocess(img)  # 3 x h x w\n",
    "        return img, torch.tensor(size).unsqueeze(0)\n",
    "\n",
    "    def _select_mask_dir(self):\n",
    "        return random.sample(self.mask_dir, 2)\n",
    "\n",
    "    def _get_mask(self, mask_path):\n",
    "        mask_array = np.array(Image.open(mask_path))\n",
    "        unique_values = np.unique(mask_array)\n",
    "        masks = {}\n",
    "\n",
    "        for value in unique_values:\n",
    "            if value not in [0, 255]:\n",
    "                # Apply binary dilation before finding connected components\n",
    "                dilated_mask = binary_dilation(mask_array == value)\n",
    "                labeled_array, num_features = label(dilated_mask)\n",
    "                for i in range(1, num_features + 1):\n",
    "                    for i in range(1, num_features + 1):\n",
    "                        mask = np.where(labeled_array == i, 1, 0)\n",
    "                        rle = mask_utils.encode(\n",
    "                            np.asfortranarray(mask.astype(np.uint8))\n",
    "                        )\n",
    "                        rle[\"counts\"] = rle[\"counts\"].decode(\n",
    "                            \"utf-8\"\n",
    "                        )  # Convert bytes to string\n",
    "                        rle_mask_key = f\"{value}_{i}\"\n",
    "                        masks[rle_mask_key] = rle\n",
    "        return masks\n",
    "\n",
    "    def _get_gt(self, mask, labels):\n",
    "        masks = {}\n",
    "        for mask_dir in self._select_mask_dir():\n",
    "            mask_path = os.path.join(mask_dir, mask + \".png\")\n",
    "            if os.path.isfile(mask_path):\n",
    "                print(mask_path)\n",
    "                masks = self._get_mask(mask_path)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        decoded_masks = {k: mask_utils.decode(v) for k, v in masks.items()}\n",
    "        print(decoded_masks)\n",
    "        for k,v in decoded_masks.items():\n",
    "            self.\n",
    "            \n",
    "        if self.preprocess:\n",
    "            decoded_masks = [\n",
    "                {k: self.preprocess(v) for k, v in mask.items()}\n",
    "                for mask in decoded_masks\n",
    "            ]\n",
    "        print(decoded_masks)\n",
    "        return decoded_masks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(os.path.join(self.img_dir, self._read_image_ids()[idx] + \".jpg\"))\n",
    "        image, size = self._get_image(\n",
    "            os.path.join(self.img_dir, self._read_image_ids()[idx] + \".jpg\")\n",
    "        )\n",
    "        gt = self._get_gt(\n",
    "            self._read_image_ids()[idx],\n",
    "            self._get_label(self._read_image_ids()[idx]),\n",
    "        )\n",
    "        print(image.shape, size, gt)\n",
    "        return {\n",
    "            BatchKeys.IMAGES: image,\n",
    "            BatchKeys.DIMS: size,\n",
    "        }, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from label_anything.data.transforms import CustomNormalize, CustomResize\n",
    "from torchvision import transforms\n",
    "annotations = '/home/emanuele/LabelAnything/data/raw/VOCdevkit/VOC2012/ImageSets/SegmentationAug/trainval_aug.txt'\n",
    "img_dir = \"/home/emanuele/LabelAnything/data/raw/VOCdevkit/VOC2012/JPEGImages\"\n",
    "mask_dir_default = (\n",
    "    \"/home/emanuele/LabelAnything/data/raw/VOCdevkit/VOC2012/SegmentationClass\"\n",
    ")\n",
    "mask_dir_aug = (\n",
    "    \"/home/emanuele/LabelAnything/data/raw/VOCdevkit/VOC2012/SegmentationClassAug\"\n",
    ")\n",
    "root = '/home/emanuele/LabelAnything/data/raw/VOCdevkit/VOC2012/'\n",
    "preprocess = transforms.Compose(\n",
    "    [\n",
    "        CustomResize(1024),\n",
    "        transforms.PILToTensor(),\n",
    "        CustomNormalize(),\n",
    "    ]\n",
    ")\n",
    "dataset = PascalVOCTestDataset(\n",
    "    root, \n",
    "    img_dir,\n",
    "    annotations,\n",
    "    mask_dir_default,\n",
    "    mask_dir_aug,\n",
    "    preprocess,\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/emanuele/LabelAnything/data/raw/VOCdevkit/VOC2012/JPEGImages/2008_006570.jpg\n",
      "/home/emanuele/LabelAnything/data/raw/VOCdevkit/VOC2012/SegmentationClassAug/2008_006570.png\n",
      "{'15_1': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), '15_2': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), '15_3': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), '17_1': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[142], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/LabelAnything/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/LabelAnything/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/LabelAnything/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/LabelAnything/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[137], line 151\u001b[0m, in \u001b[0;36mPascalVOCTestDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_image_ids()[idx] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    148\u001b[0m image, size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_image(\n\u001b[1;32m    149\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_image_ids()[idx] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m )\n\u001b[0;32m--> 151\u001b[0m gt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_gt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_image_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_image_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape, size, gt)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    157\u001b[0m     BatchKeys\u001b[38;5;241m.\u001b[39mIMAGES: image,\n\u001b[1;32m    158\u001b[0m     BatchKeys\u001b[38;5;241m.\u001b[39mDIMS: size,\n\u001b[1;32m    159\u001b[0m }, gt\n",
      "Cell \u001b[0;32mIn[137], line 139\u001b[0m, in \u001b[0;36mPascalVOCTestDataset._get_gt\u001b[0;34m(self, mask, labels)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoded_masks)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess:\n\u001b[0;32m--> 139\u001b[0m     decoded_masks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    140\u001b[0m         {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m mask\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m decoded_masks\n\u001b[1;32m    142\u001b[0m     ]\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoded_masks)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_masks\n",
      "Cell \u001b[0;32mIn[137], line 140\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoded_masks)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess:\n\u001b[1;32m    139\u001b[0m     decoded_masks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 140\u001b[0m         {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m()}\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m decoded_masks\n\u001b[1;32m    142\u001b[0m     ]\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoded_masks)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_masks\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "print(next(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(root, ids):\n",
    "    images = []\n",
    "    all_masks = []\n",
    "    all_labels = []\n",
    "\n",
    "    for image_id in ids:\n",
    "        try:\n",
    "            image = _get_images(root, image_id)\n",
    "            masks = _get_masks(root, image_id)\n",
    "            labels = _get_label(root, image_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_id}: {e}\")\n",
    "\n",
    "        images.append(image)\n",
    "        all_masks.append(masks)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    return images, all_masks, all_labels\n",
    "\n",
    "\n",
    "def _read_image_ids(image_sets_file):\n",
    "    ids = []\n",
    "    with open(image_sets_file) as f:\n",
    "        for line in f:\n",
    "            image_path, _ = line.rstrip().split(\" \")\n",
    "            image_id = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            ids.append(image_id)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def _get_images(root, image_id):\n",
    "    image_file = os.path.join(root, \"JPEGImages\", image_id + \".jpg\")\n",
    "    image = cv2.imread(str(image_file))\n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def _get_masks(root, image_id):\n",
    "    mask_array = np.array(Image.open(chosen_file))\n",
    "    unique_values = np.unique(mask_array)\n",
    "    masks = {}\n",
    "\n",
    "    for value in unique_values:\n",
    "        if value not in [0, 255]:\n",
    "            # Apply binary dilation before finding connected components\n",
    "            dilated_mask = binary_dilation(mask_array == value)\n",
    "            labeled_array, num_features = label(dilated_mask)\n",
    "            for i in range(1, num_features + 1):\n",
    "                for i in range(1, num_features + 1):\n",
    "                    mask = np.where(labeled_array == i, 1, 0)\n",
    "                    rle = mask_utils.encode(np.asfortranarray(mask.astype(np.uint8)))\n",
    "                    rle[\"counts\"] = rle[\"counts\"].decode(\n",
    "                        \"utf-8\"\n",
    "                    )  # Convert bytes to string\n",
    "                    rle_mask_key = f\"{value}_{i}\"\n",
    "                    masks[rle_mask_key] = rle\n",
    "\n",
    "    return masks\n",
    "\n",
    "\n",
    "def _get_label(root, image_id):\n",
    "    annotation_file = os.path.join(root, \"Annotations\", image_id + \".xml\")\n",
    "    objects = ET.parse(annotation_file).findall(\"object\")\n",
    "    labels = []\n",
    "\n",
    "    for object in objects:\n",
    "        class_name = object.find(\"name\").text.lower().strip()\n",
    "        labels.append(class_name)\n",
    "\n",
    "    return np.array(labels)\n",
    "\n",
    "\n",
    "# create the ids for images\n",
    "root = pathlib.Path(\"/home/emanuele/LabelAnything/data/raw/VOCdevkit/VOC2012\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12031\n"
     ]
    }
   ],
   "source": [
    "dataset_file = os.path.join(root, \"ImageSets\", \"SegmentationAug\", \"trainval_aug.txt\")\n",
    "ids = _read_image_ids(dataset_file)\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(281, 500, 3)\n",
      "/home/emanuele/LabelAnything/data/raw/VOCdevkit/VOC2012/SegmentationClass/2007_000032.png\n",
      "[  0   1  15 255]\n",
      "{'1_1': {'size': [281, 500], 'counts': '[cT11f84M2N2N101N100000O010O10000000000003M1O1O1O00O1O1LhGFY8:2011O1O1O1O00000O10O1O1O1O1O10000000O10O100000O0M4O001O1O0O2O000001O2H8I7NoQc2'}, '1_2': {'size': [281, 500], 'counts': 'Xel01g82N2O0000001O1O0000000000000000000000000000000000000000000000000000000000000000000000001O01O00000000000000000001O01O01O000000000001O0000000000000000000000000001O0001O001O000000000001O1OO1`0^O5L3K4N2M3N2N2N1O2N2N2N2QI]Nc60XIm1e65N3N1N200O00000000O1O11O1OO1O0AeI_N^6a1cI]N]6c1eIXNI1b6h1<00O00001O100O1O1O1O2N1M3O2N1O1O1IdHnN_7Q160OC[HGe77\\\\HIe75\\\\HKg71ZHOe71\\\\HOe70\\\\HNd72]HNb72_HN`73`HLMOV75mHKM2U75lHHO5T74lHG13T77jH6W7JhH4[7LfH4Y7LgH7V7IjH8U7GlHHO5T73mHG14S75lHG22T76jHHh76YHJg75ZHKg70]H0d7N]H2U8000000O1000000000O1000O10000000000000O10000000000000000000000000000000000000000000000000000000000000O001O10000000000000O0100000000000000000O0100000000000000000000000000000O01000000000000000000000001N2NmlQ1'}, '15_1': {'size': [281, 500], 'counts': 'aWf137;L6V7BkH:M6V7BkHQ1T7POkHo0W7810100N2N0G901UO[H`0g7@[H7m7Hc0N2Nab^2'}, '15_2': {'size': [281, 500], 'counts': 'S`65c82N;E2M4L3N2N10=D4L0N2000N2cNhHV1_7]OjH@Y7=f0K3K5McVm3'}}\n",
      "['aeroplane' 'aeroplane' 'person' 'person']\n",
      "(366, 500, 3)\n",
      "/home/emanuele/LabelAnything/data/raw/VOCdevkit/VOC2012/SegmentationClass/2007_000033.png\n",
      "[  0   1 255]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m _get_images(root, \u001b[38;5;28mid\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43m_get_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask)\n\u001b[1;32m      6\u001b[0m label \u001b[38;5;241m=\u001b[39m _get_label(root, \u001b[38;5;28mid\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 59\u001b[0m, in \u001b[0;36m_get_masks\u001b[0;34m(root, image_id)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m]:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Apply binary dilation before finding connected components\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     dilated_mask \u001b[38;5;241m=\u001b[39m binary_dilation(mask_array \u001b[38;5;241m==\u001b[39m value)\n\u001b[0;32m---> 59\u001b[0m     labeled_array, num_features \u001b[38;5;241m=\u001b[39m \u001b[43mlabel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdilated_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "for id in ids:\n",
    "    image = _get_images(root, id)\n",
    "    print(image.shape)\n",
    "    mask = _get_masks(root, id)\n",
    "    print(mask)\n",
    "    label = _get_label(root, id)\n",
    "    print(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generazione items dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/emanuele/LabelAnything/data/raw/VOCdevkit/VOC2012/SegmentationClassAug/2007_000032.png\n",
      "[  0   1  15 255]\n",
      "Error processing 2007_000032: 'numpy.ndarray' object is not callable\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'masks' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m images, polygons, labels \u001b[38;5;241m=\u001b[39m \u001b[43mget_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# time consuming: 2.46 minuti\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m, in \u001b[0;36mget_items\u001b[0;34m(root, ids)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[0;32m---> 15\u001b[0m     all_masks\u001b[38;5;241m.\u001b[39mappend(\u001b[43mmasks\u001b[49m)\n\u001b[1;32m     16\u001b[0m     all_labels\u001b[38;5;241m.\u001b[39mappend(labels)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m images, all_masks, all_labels\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'masks' referenced before assignment"
     ]
    }
   ],
   "source": [
    "images, polygons, labels = get_items(root, ids)  # time consuming: 2.46 minuti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Formatting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'info': {'description': 'VOC 2012 Dataset Annotations files',\n",
       "  'version': '1.0',\n",
       "  'year': 2024,\n",
       "  'contributor': 'CILAB',\n",
       "  'date_created': '02/01/2024'},\n",
       " 'images': [],\n",
       " 'annotations': [],\n",
       " 'categories': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances_voc12 = {\n",
    "    \"info\": {\n",
    "        \"description\": \"VOC 2012 Dataset Annotations files\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"year\": 2024,\n",
    "        \"contributor\": \"CILAB\",\n",
    "        \"date_created\": \"02/01/2024\",\n",
    "    },\n",
    "    \"images\": [\n",
    "        # {\n",
    "        #     \"file_name\": \"2007_000032\",\n",
    "        #     \"url\": \"VOC2012/JPEGImages/2007_000032.jpg\",\n",
    "        #     \"height\": 281,\n",
    "        #     \"width\": 500,\n",
    "        #     \"id\": 0,\n",
    "        # },\n",
    "        # {...},\n",
    "    ],\n",
    "    \"annotations\": [\n",
    "        # {\n",
    "        #     \"segmentation\": [\n",
    "        #         [\n",
    "        #             [117.0, 89.0],\n",
    "        #             [116.0, 90.0],\n",
    "        #             [109.0, 90.0],\n",
    "        #             [107.0, 92.0],\n",
    "        #             [134.0, 171.0],\n",
    "        #             [128.0, 171.0],\n",
    "        #             [127.0, 170.0],\n",
    "        #             [127.0, 137.0],\n",
    "        #         ]\n",
    "        #     ],\n",
    "        #     \"area\": 20098.5,\n",
    "        #     \"image_id\": 40,\n",
    "        #     \"bbox\": [118.0, 176.0, 330.0, 277.0],\n",
    "        #     \"category_id\": 9,\n",
    "        #     \"id\": 64,\n",
    "        # },\n",
    "        # {...},\n",
    "    ],\n",
    "    \"categories\": [\n",
    "        # {\"id\": 0, \"name\": \"sheep\"},\n",
    "        # {\"id\": 1, \"name\": \"bird\"},\n",
    "        # {\"id\": 2, \"name\": \"bus\"},\n",
    "        # {\"id\": 3, \"name\": \"cow\"},\n",
    "    ],\n",
    "}\n",
    "instances_voc12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create lvis style annotations for voc12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotation(ids, images, boxes, rle_masks, labels, annotations):\n",
    "    # generate set of categories\n",
    "    annotations_images = []\n",
    "    annotations_segmentations = []\n",
    "\n",
    "    annotations_categories = [\n",
    "        {\"id\": i, \"name\": name} for i, name in enumerate(set(np.concatenate(labels)))\n",
    "    ]\n",
    "    category_to_id = {\n",
    "        category[\"name\"]: category[\"id\"] for category in annotations_categories\n",
    "    }\n",
    "\n",
    "    for enum, id_ in enumerate(ids):\n",
    "        # print(ids[i])\n",
    "        image = {\n",
    "            \"file_name\": f\"JPEGImages/{id_}.jpg\",  # This is the only field that is compulsory\n",
    "            \"coco_url\": f\"JPEGImages/{id_}.jpg\",\n",
    "            \"height\": images[enum].shape[0],\n",
    "            \"width\": images[enum].shape[1],\n",
    "            \"id\": id_,\n",
    "        }\n",
    "        annotations_images.append(image)\n",
    "\n",
    "    i = 0\n",
    "    for enum, (id_, box, rle, label) in enumerate(zip(ids, boxes, rle_masks, labels)):\n",
    "        for b, (_, rle_value), l in zip(box, rle.items(), label):\n",
    "            annotation = {\n",
    "                \"segmentation\": rle_value,\n",
    "                \"area\": int(mask_utils.area(rle_value)),\n",
    "                \"image_id\": id_,\n",
    "                \"bbox\": b.tolist(),  # Assuming box is a list/array of [x_min, y_min, x_max, y_max]\n",
    "                \"category_id\": category_to_id[l],\n",
    "                \"id\": i,\n",
    "            }\n",
    "            annotations_segmentations.append(annotation)\n",
    "            i += 1\n",
    "\n",
    "    annotations[\"images\"] = annotations_images\n",
    "    annotations[\"annotations\"] = annotations_segmentations\n",
    "    annotations[\"categories\"] = annotations_categories\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate file, if you want to use it in the future\n",
    "annotations = create_annotation(ids, images, boxes, polygons, labels, instances_voc12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0, 'name': 'aeroplane'},\n",
       " {'id': 1, 'name': 'person'},\n",
       " {'id': 2, 'name': 'dog'},\n",
       " {'id': 3, 'name': 'bicycle'},\n",
       " {'id': 4, 'name': 'sheep'},\n",
       " {'id': 5, 'name': 'sofa'},\n",
       " {'id': 6, 'name': 'horse'},\n",
       " {'id': 7, 'name': 'chair'},\n",
       " {'id': 8, 'name': 'cow'},\n",
       " {'id': 9, 'name': 'cat'},\n",
       " {'id': 10, 'name': 'tvmonitor'},\n",
       " {'id': 11, 'name': 'bus'},\n",
       " {'id': 12, 'name': 'motorbike'},\n",
       " {'id': 13, 'name': 'train'},\n",
       " {'id': 14, 'name': 'bird'},\n",
       " {'id': 15, 'name': 'bottle'},\n",
       " {'id': 16, 'name': 'boat'},\n",
       " {'id': 17, 'name': 'diningtable'},\n",
       " {'id': 18, 'name': 'car'},\n",
       " {'id': 19, 'name': 'pottedplant'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [104.0, 105.0, 272.0, 79.0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maschera = annotations[\"annotations\"][0][\"segmentation\"]\n",
    "label = annotations[\"annotations\"][0][\"category_id\"]\n",
    "maschera_array = mask_utils.decode(maschera)\n",
    "\n",
    "bbox = annotations[\"annotations\"][0][\"bbox\"]\n",
    "print(label, bbox)\n",
    "\n",
    "# Plot della maschera and bbox\n",
    "plt.imshow(maschera_array, cmap=\"gray\")\n",
    "plt.gca().add_patch(\n",
    "    plt.Rectangle(\n",
    "        (bbox[0], bbox[1]),\n",
    "        bbox[2],\n",
    "        bbox[3],\n",
    "        linewidth=0.5,\n",
    "        edgecolor=\"r\",\n",
    "        facecolor=\"none\",\n",
    "    )\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentation <class 'str'>\n",
      "area <class 'int'>\n",
      "image_id <class 'int'>\n",
      "bbox <class 'list'>\n",
      "category_id <class 'int'>\n",
      "id <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "for k, v in annotations.get(\"annotations\")[0].items():\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save file json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"instances_voc12.json\", \"w\") as file:\n",
    "    json.dump(annotations, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/emanuele/Dottorato/dataset-vari/VOC12/JPEGImages/2007_000042.jpg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"license\": 4,\n",
    "    \"file_name\": \"000000397133.jpg\",\n",
    "    \"coco_url\": \"http://images.cocodataset.org/val2017/000000397133.jpg\",\n",
    "    \"height\": 427,\n",
    "    \"width\": 640,\n",
    "    \"date_captured\": \"2013-11-14 17:02:52\",\n",
    "    \"flickr_url\": \"http://farm7.staticflickr.com/6116/6255196340_da26cf2c9e_z.jpg\",\n",
    "    \"id\": 397133,\n",
    "}\n",
    "image = {\n",
    "    \"file_name\": \"2007_000042\",\n",
    "    \"url\": \"JPEGImages/2007_000042.jpg\",\n",
    "    \"height\": 335,\n",
    "    \"width\": 500,\n",
    "    \"id\": 3,\n",
    "}\n",
    "dataset_path = \"/home/emanuele/Dottorato/dataset-vari/VOC12\"\n",
    "(f'{dataset_path}/{image[\"url\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
