{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bf33997",
   "metadata": {},
   "source": [
    "# demo.ipynb を改変\n",
    "\n",
    "ここでは、\n",
    "\n",
    "アノテーション（point, bbox, mask）全てを画像で与えるコードを作成する\n",
    "\n",
    "demo.ipynbではpoint, bboxは座標としてコードの中で明示的に与えられていた。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad28fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee0de16",
   "metadata": {},
   "source": [
    "# マスクも含めて “すべて JSON” から読む版のフルコードです。\n",
    "マスクは JSON 内で RLE（COCO形式）または polygons で与えられる想定にしました（PNG等の外部画像は使いません）。RLE を使う場合は pycocotools が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee64caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rinotsuka/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "sys.path.append(str(Path.cwd().parent / 'label_anything'))\n",
    "\n",
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Pretty visualizations of tensors\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "from lovely_numpy import lo\n",
    "from torchvision.transforms.functional import resize\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from label_anything import LabelAnything\n",
    "from label_anything.data import get_preprocessing, utils\n",
    "from label_anything.data.transforms import PromptsProcessor\n",
    "\n",
    "# -----------------------------\n",
    "# 可視化ユーティリティ\n",
    "# -----------------------------\n",
    "colors = [\n",
    "    (255, 255, 0),   # yellow\n",
    "    (255, 0, 0),     # red\n",
    "    (0, 255, 0),     # green\n",
    "    (0, 0, 255),     # blue\n",
    "    (255, 0, 255),   # purple\n",
    "    (0, 255, 255),   # cyan\n",
    "    (255, 165, 0),   # orange\n",
    "    (255, 192, 203), # pink\n",
    "    (139, 69, 19),   # brown\n",
    "    (128, 128, 128), # grey\n",
    "    (0, 0, 0),       # black\n",
    "] * 3\n",
    "\n",
    "def draw_masks(img: Image.Image, masks: torch.Tensor, colors):\n",
    "    masked_image = resize(img.copy(), 256)\n",
    "    for i, mask in enumerate(masks):\n",
    "        mask = mask.numpy()\n",
    "        masked_image = np.where(np.repeat(mask[:, :, np.newaxis], 3, axis=2),\n",
    "                                np.asarray(colors[i], dtype=\"uint8\"),\n",
    "                                masked_image)\n",
    "    masked_image = masked_image.astype(np.uint8)\n",
    "    return cv2.addWeighted(np.array(resize(img, 256)), 0.3, masked_image, 0.7, 0)\n",
    "\n",
    "def draw_boxes(img: Image.Image, boxes: torch.Tensor, colors):\n",
    "    img = np.array(img)\n",
    "    for i, cat in enumerate(boxes):\n",
    "        for box in cat:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            img = cv2.rectangle(img, (x1, y1), (x2, y2), colors[i], 2)\n",
    "    return img\n",
    "\n",
    "def draw_points(img: Image.Image, points: torch.Tensor, colors):\n",
    "    img = np.array(img)\n",
    "    for i, cat in enumerate(points):\n",
    "        for point in cat:\n",
    "            x, y = map(int, point)\n",
    "            img = cv2.circle(img, (x, y), 5, colors[i], -1)\n",
    "    return img\n",
    "\n",
    "def draw_all(img: Image.Image, masks, boxes, points, colors):\n",
    "    segmented_image = draw_masks(img, masks, colors)\n",
    "    img = Image.fromarray(segmented_image)\n",
    "    img = resize(img, 1024)\n",
    "    img = draw_boxes(img, boxes, colors)\n",
    "    img = Image.fromarray(img)\n",
    "    img = draw_points(img, points, colors)\n",
    "    img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "def get_image(image_tensor: torch.Tensor) -> Image.Image:\n",
    "    MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "    STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "    unnormalized_image = (image_tensor.numpy() * np.array(STD)[:, None, None]) + np.array(MEAN)[:, None, None]\n",
    "    unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "    unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "    return Image.fromarray(unnormalized_image)\n",
    "\n",
    "def dict_to_device(d, device):\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            d[k] = v.to(device)\n",
    "        elif isinstance(v, dict):\n",
    "            d[k] = dict_to_device(v, device)\n",
    "        elif isinstance(v, list):\n",
    "            d[k] = [dict_to_device(vv, device) for vv in v]\n",
    "    return d\n",
    "\n",
    "# -----------------------------\n",
    "# すべて JSON 読み込み関連（RLE/Polygonをサポート）\n",
    "# -----------------------------\n",
    "ANNOT_DIR = Path.cwd() / \"annotations\"\n",
    "CLASS_NAME_TO_ID = {\n",
    "    \"handrail\": 1,\n",
    "    \"midrail\": 2,\n",
    "    \"baseboard\": 3,\n",
    "    # 日本語名を使うならここに追記可\n",
    "    \"手すり\": 1,\n",
    "    \"中桟\": 2,\n",
    "    \"巾木\": 3,\n",
    "}\n",
    "\n",
    "try:\n",
    "    from pycocotools import mask as mask_utils\n",
    "except Exception:\n",
    "    mask_utils = None\n",
    "\n",
    "def _parse_class_key(k: Any, class_map: Dict[str, int]) -> int:\n",
    "    \"\"\"\n",
    "    JSON のクラスキーを int に正規化。\n",
    "    - 数字文字列や数値は int に\n",
    "    - 文字列で handrail/midrail/toeboard（または classes で定義）なら対応 ID に\n",
    "    \"\"\"\n",
    "    if isinstance(k, int):\n",
    "        return k\n",
    "    if isinstance(k, str):\n",
    "        ks = k.strip().lower()\n",
    "        if ks.isdigit():\n",
    "            return int(ks)\n",
    "        if ks in class_map:\n",
    "            return class_map[ks]\n",
    "    raise ValueError(f\"Unsupported class key: {k}\")\n",
    "\n",
    "def _mask_from_polygons(size_hw: List[int], polygons: List[List[float]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    polygons: [[x1,y1,x2,y2,...], ...]\n",
    "    size_hw: [H, W]\n",
    "    return: 0/255 の 2値マスク (H, W)\n",
    "    \"\"\"\n",
    "    H, W = int(size_hw[0]), int(size_hw[1])\n",
    "    m = np.zeros((H, W), dtype=np.uint8)\n",
    "    for poly in polygons:\n",
    "        if not poly:\n",
    "            continue\n",
    "        pts = np.array(poly, dtype=np.float32).reshape(-1, 2)\n",
    "        pts = np.round(pts).astype(np.int32)\n",
    "        cv2.fillPoly(m, [pts], 255)\n",
    "    return m\n",
    "\n",
    "def _mask_from_rle(size_hw: List[int], counts) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    COCO RLE をデコードして 0/255 の 2値マスクに。\n",
    "    counts: 圧縮文字列 or 非圧縮リスト（どちらも COCO 互換）\n",
    "    pycocotools が無ければ RLE は使えない（polygons を使ってください）。\n",
    "    \"\"\"\n",
    "    if mask_utils is None:\n",
    "        raise RuntimeError(\n",
    "            \"RLE マスクを使うには pycocotools が必要です。`pip install pycocotools` をインストールするか、\"\n",
    "            \"JSON の masks を polygons 形式にしてください。\"\n",
    "        )\n",
    "    rle = {\"size\": [int(size_hw[0]), int(size_hw[1])], \"counts\": counts}\n",
    "    m = mask_utils.decode(rle)  # (H,W) or (H,W,1)\n",
    "    if m.ndim == 3:\n",
    "        m = m[..., 0]\n",
    "    return (m.astype(np.uint8) * 255)\n",
    "\n",
    "def _load_mask_entry_json(entry: Any) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    entry:\n",
    "      - {\"format\":\"rle\", \"size\":[H,W], \"counts\":\"<COCO compressed string>\"}\n",
    "      - {\"format\":\"rle\", \"size\":[H,W], \"counts\":[...]}  # 非圧縮 RLE リスト\n",
    "      - {\"format\":\"polygons\", \"size\":[H,W], \"polygons\":[[x1,y1,...], ...]}\n",
    "    return: 0/255 の 2値マスク (H, W)\n",
    "    \"\"\"\n",
    "    if not isinstance(entry, dict):\n",
    "        raise ValueError(f\"Mask entry must be dict, got {type(entry)}\")\n",
    "\n",
    "    fmt = str(entry.get(\"format\", \"\")).lower()\n",
    "    size = entry.get(\"size\", None)\n",
    "    if not (isinstance(size, (list, tuple)) and len(size) == 2):\n",
    "        raise ValueError(f\"Mask entry missing valid 'size': {entry}\")\n",
    "\n",
    "    if fmt == \"polygons\":\n",
    "        polygons = entry.get(\"polygons\", [])\n",
    "        return _mask_from_polygons(size, polygons)\n",
    "\n",
    "    if fmt == \"rle\":\n",
    "        counts = entry.get(\"counts\", None)\n",
    "        if counts is None:\n",
    "            raise ValueError(f\"RLE entry missing 'counts': {entry}\")\n",
    "        return _mask_from_rle(size, counts)\n",
    "\n",
    "    raise ValueError(f\"Unsupported mask format: {fmt}\")\n",
    "\n",
    "def load_annotations_for_image(stem: str) -> Dict[str, Dict[int, list]]:\n",
    "    \"\"\"\n",
    "    annotations/<stem>.json を読み込み、\n",
    "      - 'bboxes' : {class_id: [[x1,y1,x2,y2], ...]}\n",
    "      - 'points' : {class_id: [[x,y], ...]}\n",
    "      - 'masks'  : {class_id: [np.ndarray(H,W;0/255), ...]}  ← JSON 内の RLE/Polygon を 2値マスクに変換\n",
    "    の形で返す。ファイルが無ければ空辞書を返す。\n",
    "    \"\"\"\n",
    "    ann_path = ANNOT_DIR / f\"{stem}.json\"\n",
    "    result = {\"bboxes\": {}, \"points\": {}, \"masks\": {}}\n",
    "    if not ann_path.exists():\n",
    "        return result\n",
    "\n",
    "    with open(ann_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # classes があればマージ（優先: ファイル）\n",
    "    class_map = dict(CLASS_NAME_TO_ID)\n",
    "    if \"classes\" in data and isinstance(data[\"classes\"], dict):\n",
    "        for name, cid in data[\"classes\"].items():\n",
    "            try:\n",
    "                class_map[str(name).strip().lower()] = int(cid)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # bboxes / points をクラスIDキーに正規化\n",
    "    for k in [\"bboxes\", \"points\"]:\n",
    "        raw = data.get(k, {})\n",
    "        if isinstance(raw, dict):\n",
    "            for kk, vv in raw.items():\n",
    "                cid = _parse_class_key(kk, class_map)\n",
    "                if not isinstance(vv, list):\n",
    "                    vv = []\n",
    "                result[k][cid] = vv\n",
    "\n",
    "    # masks: JSON内の各エントリを実マスク(np.ndarray)に変換\n",
    "    raw_masks = data.get(\"masks\", {})\n",
    "    if isinstance(raw_masks, dict):\n",
    "        for kk, entries in raw_masks.items():\n",
    "            cid = _parse_class_key(kk, class_map)\n",
    "            masks_np = []\n",
    "            if isinstance(entries, list):\n",
    "                for entry in entries:\n",
    "                    m = _load_mask_entry_json(entry)\n",
    "                    if m is not None:\n",
    "                        masks_np.append(m.astype(np.uint8))\n",
    "            result[\"masks\"][cid] = masks_np\n",
    "\n",
    "    return result\n",
    "\n",
    "def union_class_ids(dicts_per_support: List[Dict[str, Dict[int, list]]]) -> List[int]:\n",
    "    s = set()\n",
    "    for d in dicts_per_support:\n",
    "        for k in [\"bboxes\", \"points\", \"masks\"]:\n",
    "            s |= set(d.get(k, {}).keys())\n",
    "    return sorted(list(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72801e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# モデル/前処理 準備\n",
    "# -----------------------------\n",
    "image_size = 1024\n",
    "custom_preprocess = True\n",
    "\n",
    "accelerator = Accelerator(cpu=True)\n",
    "device = accelerator.device\n",
    "\n",
    "# Label Anything 本体\n",
    "la = LabelAnything.from_pretrained(\"pasqualedem/label_anything_sam_1024_coco\")\n",
    "\n",
    "# 画像ロード\n",
    "img_dir = Path.cwd() / \"images\"\n",
    "img_paths = sorted(list(img_dir.glob(\"*.jpg\")) + list(img_dir.glob(\"*.jpeg\")) +\n",
    "                   list(img_dir.glob(\"*.png\")) + list(img_dir.glob(\"*.JPG\")) + list(img_dir.glob(\"*.PNG\")))\n",
    "assert len(img_paths) >= 2, \"画像はクエリ1 + サポート1以上が必要です（./images に配置）。\"\n",
    "\n",
    "def open_rgb(p: Path) -> Image.Image:\n",
    "    return Image.open(p).convert(\"RGB\")\n",
    "\n",
    "# 先頭=クエリ、残り=サポート\n",
    "query_orig = open_rgb(img_paths[0])\n",
    "support_orig_images = [open_rgb(p) for p in img_paths[1:]]\n",
    "\n",
    "# 前処理（LA の transform）\n",
    "preprocess = get_preprocessing(\n",
    "    {\n",
    "        \"common\": {\n",
    "            \"custom_preprocess\": custom_preprocess,\n",
    "            \"image_size\": image_size,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "query_image = preprocess(query_orig)\n",
    "support_images = [preprocess(img) for img in support_orig_images]\n",
    "\n",
    "# 画像サイズ（LA内部の tensor 化/正規化用）\n",
    "support_sizes: List[Tuple[int, int]] = [img.size for img in support_orig_images]   # (W,H)\n",
    "all_sizes: List[Tuple[int, int]] = [query_orig.size] + support_sizes               # DIMS 用\n",
    "\n",
    "# プロンプト前処理器\n",
    "prompts_processor = PromptsProcessor(\n",
    "    long_side_length=image_size,\n",
    "    masks_side_length=256,\n",
    "    custom_preprocess=custom_preprocess,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4473d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bboxes: torch.Size([2, 4, 0, 4])\n",
      "points: torch.Size([2, 4, 0, 2])\n",
      "masks: torch.Size([2, 4, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# JSON から bbox/point/mask を構築（すべて JSON）\n",
    "# -----------------------------\n",
    "# 1) 各サポート画像について annotations/<stem>.json を読む\n",
    "support_annots: List[Dict[str, Dict[int, list]]] = []\n",
    "for p in img_paths[1:]:\n",
    "    support_annots.append(load_annotations_for_image(p.stem))\n",
    "\n",
    "# 2) 全サポートで現れるクラスIDの集合を抽出（背景 -1 は後で付与）\n",
    "cat_ids = union_class_ids(support_annots)\n",
    "if not cat_ids:\n",
    "    # どれも空なら、念のため既定の3クラスを用意（handrail, midrail, toeboard）\n",
    "    cat_ids = [1, 2, 3]\n",
    "cat_ids = sorted(cat_ids)\n",
    "\n",
    "# 3) bboxes/points/masks を Label Anything 期待の形に揃える\n",
    "#    形: List[ support画像ごとに { class_id: List[...] } ]\n",
    "bboxes_list: List[Dict[int, List[List[float]]]] = []\n",
    "points_list: List[Dict[int, List[List[float]]]] = []\n",
    "masks_list:  List[Dict[int, List[np.ndarray]]] = []\n",
    "\n",
    "for ann in support_annots:\n",
    "    per_img_b = {}\n",
    "    per_img_p = {}\n",
    "    per_img_m = {}\n",
    "\n",
    "    # すべてのクラスについてキーを保証\n",
    "    for cid in cat_ids:\n",
    "        per_img_b[cid] = list(ann.get(\"bboxes\", {}).get(cid, []))\n",
    "        per_img_p[cid] = list(ann.get(\"points\", {}).get(cid, []))\n",
    "        per_img_m[cid] = list(ann.get(\"masks\",  {}).get(cid, []))  # ここはすでに np.ndarray のリスト\n",
    "\n",
    "    bboxes_list.append(per_img_b)\n",
    "    points_list.append(per_img_p)\n",
    "    masks_list.append(per_img_m)\n",
    "\n",
    "# 4) bbox は LA 形式に変換（正規化など）\n",
    "#    convert_bbox は (x1,y1,x2,y2, *image.size) を前提\n",
    "converted_bboxes: List[Dict[int, List[List[float]]]] = []\n",
    "for img_bboxes, orig_img in zip(bboxes_list, support_orig_images):\n",
    "    out = {}\n",
    "    for cid, cat_bboxes in img_bboxes.items():\n",
    "        out[cid] = [\n",
    "            prompts_processor.convert_bbox(bbox, *orig_img.size, noise=False)\n",
    "            for bbox in cat_bboxes\n",
    "        ]\n",
    "    converted_bboxes.append(out)\n",
    "\n",
    "# 5) 背景 -1 を追加（空）\n",
    "bboxes_list_bg = [{**{-1: []}, **bb} for bb in converted_bboxes]\n",
    "points_list_bg = [{**{-1: []}, **pp} for pp in points_list]\n",
    "masks_list_bg  = [{**{-1: []}, **mm} for mm in masks_list]\n",
    "cat_ids_with_bg = [-1] + cat_ids\n",
    "\n",
    "# 6) numpy 配列化（クラスごとに空でも配列化）\n",
    "for i in range(len(bboxes_list_bg)):\n",
    "    for cid in cat_ids_with_bg:\n",
    "        bboxes_list_bg[i][cid] = np.array(bboxes_list_bg[i][cid], dtype=np.float32)\n",
    "        points_list_bg[i][cid] = np.array(points_list_bg[i][cid], dtype=np.float32)\n",
    "        # masks はすでに np.ndarray のリスト（LA 側で 256 へ整形）\n",
    "\n",
    "# 7) tensor 化（スタック＆パディング）※ img_sizes は「サポート画像」サイズを渡す\n",
    "bboxes, flag_bboxes = utils.annotations_to_tensor(\n",
    "    prompts_processor, bboxes_list_bg, support_sizes, utils.PromptType.BBOX\n",
    ")\n",
    "points, flag_points = utils.annotations_to_tensor(\n",
    "    prompts_processor, points_list_bg, support_sizes, utils.PromptType.POINT\n",
    ")\n",
    "masks,  flag_masks  = utils.annotations_to_tensor(\n",
    "    prompts_processor, masks_list_bg,  support_sizes, utils.PromptType.MASK\n",
    ")\n",
    "\n",
    "# 各クラスが存在するかの統合フラグ\n",
    "flag_examples = utils.flags_merge(\n",
    "    flag_bboxes=flag_bboxes, flag_points=flag_points, flag_masks=flag_masks\n",
    ")\n",
    "\n",
    "print(\"bboxes:\", bboxes.shape)       # (num_support, num_classes+1, num_bboxes, 4)\n",
    "print(\"points:\", points.shape)       # (num_support, num_classes+1, num_points, 2)\n",
    "print(\"masks:\", masks.shape)         # (num_support, num_classes+1, 256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490e5174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== sanity check ===\n",
      "[support 0] #bbox=0 #point=0 #mask=0 classes=[1, 2, 3]\n",
      "[support 1] #bbox=0 #point=0 #mask=0 classes=[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# debug: 読み込んだ内容のサニティチェック\n",
    "# ここを入れると、読み込んだ bboxes_list / points_list / masks_list の内容がわかる  \n",
    "\n",
    "# JSONを読み終えて bboxes_list / points_list / masks_list を作った直後に入れる\n",
    "\n",
    "print(\"=== sanity check ===\")\n",
    "for idx, (b,p,m) in enumerate(zip(bboxes_list, points_list, masks_list)):\n",
    "    tb = sum(len(v) for v in b.values())\n",
    "    tp = sum(len(v) for v in p.values())\n",
    "    tm = sum(len(v) for v in m.values())\n",
    "    print(f\"[support {idx}] #bbox={tb} #point={tp} #mask={tm} classes={sorted(set(b)|set(p)|set(m))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048972ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for: ['image02', 'image03']\n",
      "exists? /Users/rinotsuka/code/papers/LabelAnything/LabelAnything/notebooks/annotations/image02.json False\n",
      "exists? /Users/rinotsuka/code/papers/LabelAnything/LabelAnything/notebooks/annotations/image03.json False\n"
     ]
    }
   ],
   "source": [
    "# debug\n",
    "# 該当 JSON の存在と中身を確認\n",
    "from pathlib import Path, PurePath\n",
    "import json\n",
    "\n",
    "print(\"looking for:\", [p.stem for p in img_paths[1:]])\n",
    "for p in img_paths[1:]:\n",
    "    ann_path = ANNOT_DIR / f\"{p.stem}.json\"\n",
    "    print(\"exists?\", ann_path, ann_path.exists())\n",
    "    if ann_path.exists():\n",
    "        try:\n",
    "            data = json.load(open(ann_path, \"r\", encoding=\"utf-8\"))\n",
    "            print(\"keys:\", list(data.keys()))\n",
    "            print(\"bboxes sample:\", str(data.get(\"bboxes\", \"\"))[:120])\n",
    "            print(\"points sample:\", str(data.get(\"points\", \"\"))[:120])\n",
    "            print(\"masks sample:\", str(data.get(\"masks\", \"\"))[:120])\n",
    "        except Exception as e:\n",
    "            print(\"json load error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23309fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b28cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63808352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e928a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 可視化（任意）\n",
    "# -----------------------------\n",
    "drawn_images = [\n",
    "    draw_all(get_image(img_t), img_masks, img_bboxes, img_points, colors)\n",
    "    for img_t, img_masks, img_bboxes, img_points in zip(\n",
    "        support_images, masks, bboxes, points\n",
    "    )\n",
    "]\n",
    "# Image.fromarray(drawn_images[0]).show()  # 必要なら\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98919e46",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No prompts provided",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m input_dict = dict_to_device(input_dict, device)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     output = \u001b[43mla\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m logits = output[\u001b[33m\"\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     21\u001b[39m predictions = torch.argmax(logits, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/label_anything/models/build_lam.py:508\u001b[39m, in \u001b[36mLabelAnything.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/label_anything/models/lam.py:90\u001b[39m, in \u001b[36mLam.forward\u001b[39m\u001b[34m(self, batched_input)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mself\u001b[39m, batched_input: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]\n\u001b[32m     59\u001b[39m ) -> List[Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]]:\n\u001b[32m     60\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m    Predicts masks end-to-end from provided images and prompts.\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m    If prompts are not known in advance, using SamPredictor is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     88\u001b[39m \u001b[33;03m            original size of the image.\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     seg, pe_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     seg = \u001b[38;5;28mself\u001b[39m.postprocess_masks(seg, batched_input[\u001b[33m\"\u001b[39m\u001b[33mdims\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mflag_gts\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batched_input:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/label_anything/models/lam.py:121\u001b[39m, in \u001b[36mLam._forward\u001b[39m\u001b[34m(self, batched_input)\u001b[39m\n\u001b[32m    116\u001b[39m query_embeddings, prompt_embeddings = \u001b[38;5;28mself\u001b[39m.prepare_query_example_embeddings(\n\u001b[32m    117\u001b[39m     batched_input\n\u001b[32m    118\u001b[39m )\n\u001b[32m    119\u001b[39m points, boxes, masks, flag_examples = \u001b[38;5;28mself\u001b[39m.prepare_prompts(batched_input)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m pe_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprompt_encoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflag_examples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflag_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m seg = \u001b[38;5;28mself\u001b[39m.mask_decoder(\n\u001b[32m    130\u001b[39m     query_embeddings=query_embeddings,\n\u001b[32m    131\u001b[39m     support_embeddings=prompt_embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m    134\u001b[39m     flag_examples=flag_examples,\n\u001b[32m    135\u001b[39m )\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m seg, pe_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/label_anything/models/prompt_encoder.py:779\u001b[39m, in \u001b[36mPromptImageEncoder.forward\u001b[39m\u001b[34m(self, image_embeddings, points, boxes, masks, flag_examples, chunk_size)\u001b[39m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    753\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    754\u001b[39m     image_embeddings: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    759\u001b[39m     chunk_size=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    760\u001b[39m ) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[32m    761\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    762\u001b[39m \u001b[33;03m    Embeds different types of prompts, returning class embeddings\u001b[39;00m\n\u001b[32m    763\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    777\u001b[39m \u001b[33;03m        Bx(embed_dim)x(embed_H)x(embed_W)\u001b[39;00m\n\u001b[32m    778\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m     sparse_embeddings, dense_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_points_masks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m     sparse_embeddings = rearrange(sparse_embeddings, \u001b[33m\"\u001b[39m\u001b[33mb m c n d -> (b m c) n d\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    784\u001b[39m     b, m, c, d, h, w = dense_embeddings.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/label_anything/models/prompt_encoder.py:589\u001b[39m, in \u001b[36mPromptImageEncoder.embed_points_masks\u001b[39m\u001b[34m(self, points, boxes, masks, chunk_size)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_points_masks\u001b[39m(\n\u001b[32m    565\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    566\u001b[39m     points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n\u001b[32m   (...)\u001b[39m\u001b[32m    569\u001b[39m     chunk_size: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    570\u001b[39m ) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[32m    571\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    572\u001b[39m \u001b[33;03m    Embeds different types of prompts, returning both sparse and dense\u001b[39;00m\n\u001b[32m    573\u001b[39m \u001b[33;03m    embeddings.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    587\u001b[39m \u001b[33;03m        Bx(embed_dim)x(embed_H)x(embed_W)\u001b[39;00m\n\u001b[32m    588\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m     B, n_examples, n_classes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_batch_examples_class_size\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    592\u001b[39m     bs = B * n_examples * n_classes\n\u001b[32m    594\u001b[39m     sparse_embeddings = torch.empty(\n\u001b[32m    595\u001b[39m         (bs, \u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.embed_dim),\n\u001b[32m    596\u001b[39m         device=\u001b[38;5;28mself\u001b[39m._get_device(),\n\u001b[32m    597\u001b[39m         dtype=\u001b[38;5;28mself\u001b[39m.no_sparse_embedding.weight.dtype,\n\u001b[32m    598\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/papers/LabelAnything/LabelAnything/label_anything/models/prompt_encoder.py:562\u001b[39m, in \u001b[36mPromptImageEncoder._get_batch_examples_class_size\u001b[39m\u001b[34m(self, points, boxes, masks)\u001b[39m\n\u001b[32m    560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m masks.shape[\u001b[32m0\u001b[39m], masks.shape[\u001b[32m1\u001b[39m], masks.shape[\u001b[32m2\u001b[39m]\n\u001b[32m    561\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo prompts provided\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: No prompts provided"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 推論\n",
    "# -----------------------------\n",
    "input_dict = {\n",
    "    utils.BatchKeys.IMAGES: torch.stack([query_image] + support_images).unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_BBOXES: bboxes.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_BBOXES:   flag_bboxes.unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_POINTS: points.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_POINTS:   flag_points.unsqueeze(0),\n",
    "    utils.BatchKeys.PROMPT_MASKS:  masks.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_MASKS:    flag_masks.unsqueeze(0),\n",
    "    utils.BatchKeys.FLAG_EXAMPLES: flag_examples.unsqueeze(0),\n",
    "    # DIMS は [query] + [supports] の順に (W,H)\n",
    "    utils.BatchKeys.DIMS: torch.tensor([all_sizes], dtype=torch.int32),\n",
    "}\n",
    "input_dict = dict_to_device(input_dict, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = la(input_dict)\n",
    "logits = output[\"logits\"]\n",
    "predictions = torch.argmax(logits, dim=1)\n",
    "predictions.chans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d43778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "label-anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
